{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98ddda3a-90c6-412f-bf9a-c4951235468e",
   "metadata": {},
   "source": [
    "# **Notebook 02 - Credit Risk Modeling under Autonomous Constraints**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8430057-74f6-420e-aa7f-cf1c5af50d46",
   "metadata": {},
   "source": [
    "## **Section 1 - Objective and Scientific Positioning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b71847-ddd5-4c9a-9434-47d817c9236a",
   "metadata": {},
   "source": [
    "### **1.1 Purpose of This Notebook**\n",
    "\n",
    "The objective of this notebook is to establish a **baseline credit risk modeling pipeline** and use it as a controlled environment to investigate the emergence of **autonomous risk** under realistic conditions.\n",
    "\n",
    "Rather than introducing novel prediction tasks or unconventional labels, this notebook deliberately focuses on a **standard and widely accepted problem**:\n",
    "\n",
    "> **Binary classification of credit default risk (`label_default`).**\n",
    "\n",
    "This choice is intentional and methodological. By grounding the analysis in a canonical task, we ensure that any observed risk amplification, instability, or governance failure cannot be dismissed as an artifact of problem construction.\n",
    "\n",
    "\n",
    "### **1.2 Why Credit Default as the Reference Task**\n",
    "\n",
    "Credit default prediction is one of the most mature application domains in machine learning. It is characterized by:\n",
    "\n",
    "* well-defined labels;\n",
    "* extensive regulatory oversight;\n",
    "* a large body of academic and industrial benchmarks;\n",
    "* and clear economic consequences.\n",
    "\n",
    "As such, it provides an ideal testbed to examine whether **standard performance-oriented modeling practices are sufficient** to capture all relevant forms of risk.\n",
    "\n",
    "The central question guiding this notebook is therefore:\n",
    "\n",
    "> *Can a model be statistically accurate and economically useful, yet structurally risky under autonomous deployment conditions?*\n",
    "\n",
    "\n",
    "### **1.3 From Predictive Risk to Autonomous Risk**\n",
    "\n",
    "Traditional credit risk models focus on minimizing prediction error under assumptions of:\n",
    "\n",
    "* static data distributions;\n",
    "* passive model deployment;\n",
    "* and strong human oversight.\n",
    "\n",
    "However, modern credit systems increasingly involve:\n",
    "\n",
    "* automated decision pipelines;\n",
    "* limited human review;\n",
    "* feedback effects between model decisions and future data;\n",
    "* and opaque model architectures.\n",
    "\n",
    "In such contexts, **risk is no longer fully characterized by misclassification rates**. Instead, it emerges from the interaction between:\n",
    "\n",
    "* **Autonomy (A):** how independently the model drives decisions;\n",
    "* **Opacity (O):** how difficult its reasoning is to interpret;\n",
    "* **Human Oversight (H):** how effectively humans can intervene;\n",
    "* and **systemic impact**, even when predictions are “correct.”\n",
    "\n",
    "This notebook operationalizes these ideas within a familiar credit risk setting.\n",
    "\n",
    "\n",
    "### **1.4 Scope and Boundaries**\n",
    "\n",
    "It is important to clarify what this notebook **does and does not claim**.\n",
    "\n",
    "#### **This notebook **does**:**\n",
    "\n",
    "* Train standard supervised models for default prediction;\n",
    "* Evaluate classical performance metrics (AUC, accuracy, etc.);\n",
    "* Introduce structural indicators related to autonomy and opacity;\n",
    "* Prepare the ground for later analysis of instability and feedback loops.\n",
    "\n",
    "#### **This notebook **does not**:**\n",
    "\n",
    "* Claim the existence of intent, awareness, or consciousness;\n",
    "* Attribute moral agency to the model;\n",
    "* Replace regulatory or legal definitions of creditworthiness.\n",
    "\n",
    "The focus remains strictly on **observable, measurable system behavior**.\n",
    "\n",
    "\n",
    "\n",
    "### **1.5 Relationship to the Other Notebooks**\n",
    "\n",
    "This notebook plays a **foundational role** in the project:\n",
    "\n",
    "| Notebooks | Description |\n",
    "|--------|-------------|\n",
    "| 01 | introduced the synthetic environment and feature construction; |\n",
    "| 02 | establishes a classical supervised learning baseline; |\n",
    "| 03 | will extend the analysis to fraud and anomaly-driven risk; |\n",
    "| 04 | will examine governance, opacity, and auditability; |\n",
    "| 05 | will explore feedback loops and temporal risk dynamics; |\n",
    "| 06 | will generalize the framework toward AI Safety and AGI-relevant concerns. |\n",
    "\n",
    "\n",
    "In other words:\n",
    "\n",
    "> *Notebook 02 answers the question:\n",
    "> “What does autonomous risk look like when nothing appears wrong?”*\n",
    "\n",
    "\n",
    "### **1.6 Expected Outcome**\n",
    "\n",
    "By the end of this notebook, we expect to show that:\n",
    "\n",
    "* High predictive performance does not imply low autonomous risk;\n",
    "* Structural properties of models matter independently of accuracy;\n",
    "* Standard evaluation protocols leave critical blind spots.\n",
    "\n",
    "These results will serve as a baseline reference for all subsequent analyses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1e0fed-6078-4dbf-9dd7-88a2d3f0f93e",
   "metadata": {},
   "source": [
    "## **Section 2 - Dataset and Label Construction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3f6cec-249a-4a5a-8b76-01366691b9cf",
   "metadata": {},
   "source": [
    "### **2.1 Dataset Overview**\n",
    "\n",
    "This notebook uses a **synthetic financial dataset** designed to emulate realistic credit decision environments while allowing full experimental control and reproducibility.\n",
    "\n",
    "The dataset, generated and validated in **Notebook 01**, contains **10,000 individual records**, each representing a hypothetical customer profile with demographic, financial, behavioral, and transactional attributes.\n",
    "\n",
    "The use of a synthetic dataset serves three methodological purposes:\n",
    "\n",
    "1. **Ethical control:** no real individuals are affected;\n",
    "2. **Structural transparency:** all data-generating mechanisms are known;\n",
    "3. **Experimental flexibility:** labels and regimes can be adjusted without bias leakage.\n",
    "\n",
    "Importantly, the dataset is *not* intended to perfectly replicate any specific real-world population, but to capture **structural regularities** commonly present in credit systems.\n",
    "\n",
    "\n",
    "### **2.2 Feature Categories**\n",
    "\n",
    "The dataset contains multiple classes of variables, grouped conceptually as follows:\n",
    "\n",
    "#### **(a) Financial Capacity and Exposure**\n",
    "\n",
    "Examples include:\n",
    "\n",
    "* income estimates;\n",
    "* total credit limits;\n",
    "* debt-to-income ratios;\n",
    "* utilization metrics.\n",
    "\n",
    "These variables approximate the **economic capacity** of an individual.\n",
    "\n",
    "#### **(b) Behavioral and Transactional Signals**\n",
    "\n",
    "Examples include:\n",
    "\n",
    "* transaction frequency;\n",
    "* average transaction value;\n",
    "* unusual transaction indicators;\n",
    "* international purchase flags.\n",
    "\n",
    "These variables capture **behavioral patterns** relevant to credit and fraud risk.\n",
    "\n",
    "#### **(c) Structural and Derived Features**\n",
    "\n",
    "In addition to raw variables, the dataset includes engineered features that represent **structural properties** of system interaction, such as:\n",
    "\n",
    "* interaction terms (e.g., nonlinear combinations);\n",
    "* normalized risk signals;\n",
    "* autonomy- and opacity-related constructs introduced in later notebooks.\n",
    "\n",
    "The separation between raw and derived features is crucial to avoid **label leakage** and to maintain interpretability.\n",
    "\n",
    "\n",
    "### **2.3 Target Variable: `label_default`**\n",
    "\n",
    "The target variable for this notebook is **credit default risk**, operationalized as a binary label:\n",
    "\n",
    "`label_default` \n",
    "$\\in$ \\{0, 1}\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "* `0` indicates non-default;\n",
    "* `1` indicates default.\n",
    "\n",
    "Rather than using an externally provided ground-truth label, `label_default` is **constructed from an underlying continuous risk score**, ensuring both realism and control.\n",
    "\n",
    "### **Note:**\n",
    "\n",
    "$\n",
    "y = \\texttt{label\\_defaulty}\n",
    "$\n",
    "\n",
    "> `label_default` represents a synthetic proxy for credit default risk, not an observed ground-truth event.\n",
    "\n",
    "\n",
    "\n",
    "### **2.4 Label Construction Procedure**\n",
    "\n",
    "The label is derived from the variable `prob_inadimplencia` (default probability proxy), generated during the synthetic data process.\n",
    "\n",
    "The construction follows a **quantile-based thresholding approach**:\n",
    "\n",
    "* The top **10%** of observations by `prob_inadimplencia` are labeled as defaults;\n",
    "* The remaining **90%** are labeled as non-defaults.\n",
    "\n",
    "Formally:\n",
    "\n",
    "\n",
    "$\n",
    "\\texttt{label\\_default}_i =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } p_i > Q_{0.90}(p) \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "Or rather:\n",
    "\n",
    "$\n",
    "$$\n",
    "\\text{label\\_default}_i =\n",
    "\\begin{cases}\n",
    "1, & \\text{if } \\text{prob\\_inadimplência}_i > Q_{0.90} \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "$\n",
    "\n",
    "\n",
    "where $p_i$ is the default probability proxy for observation $i$.\n",
    "\n",
    "This approach ensures:\n",
    "\n",
    "* class imbalance consistent with real credit datasets;\n",
    "* sufficient positive examples for robust modeling;\n",
    "* and stable class distributions under stratified sampling.\n",
    "\n",
    "### **Note:**\n",
    "\n",
    "* quantile-based labeling;\n",
    "* synthetic stress-test design;\n",
    "* not a claim of real default behavior.\n",
    "\n",
    "\n",
    "### **2.5 Why a Quantile-Based Label**\n",
    "\n",
    "The use of a quantile threshold is intentional and methodologically justified:\n",
    "\n",
    "* It avoids arbitrary absolute cutoffs;\n",
    "* It adapts naturally to distributional changes;\n",
    "* It preserves ordinal risk structure;\n",
    "* It facilitates controlled experiments across notebooks.\n",
    "\n",
    "Most importantly, it ensures that **the label encodes risk, not behavior induced by the model itself**, preventing circularity.\n",
    "\n",
    "\n",
    "### **2.6 Class Balance and Validation Checks**\n",
    "\n",
    "Before any modeling, the dataset is validated to ensure:\n",
    "\n",
    "* both classes are present;\n",
    "* the class ratio is within expected bounds;\n",
    "* no degenerate label distributions occur after splitting.\n",
    "\n",
    "These checks are essential to prevent **silent modeling failures** that could invalidate downstream risk analysis.\n",
    "\n",
    "\n",
    "### **2.7 Scope Limitation**\n",
    "\n",
    "At this stage:\n",
    "\n",
    "* the label captures **credit default risk only;**\n",
    "* no fraud or anomaly labels are introduced;\n",
    "* no feedback effects are applied.\n",
    "\n",
    "This controlled setup allows us to establish a **clean supervised baseline** before introducing autonomy-driven complexity in later notebooks.\n",
    "\n",
    "\n",
    "### **2.8 Transition to Modeling**\n",
    "\n",
    "With the dataset and label defined, the next step is to:\n",
    "\n",
    "* select theory-consistent feature sets;\n",
    "* construct training and test splits;\n",
    "* and train baseline supervised models.\n",
    "\n",
    "These steps are addressed in the next section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9832d6ac-3f23-4e4f-a8c7-a2eb8ca39109",
   "metadata": {},
   "source": [
    "## **Section 3 - Feature Selection and Train/Test Split**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4718fe2-39e0-4ef9-a8ca-48fd3354baa6",
   "metadata": {},
   "source": [
    "### **3.1 Objective of Feature Selection**\n",
    "\n",
    "The purpose of this section is to define the set of explanatory variables used for supervised credit risk modeling, while ensuring:\n",
    "\n",
    "* Absence of prohibited or sensitive attributes (e.g., gender, race);\n",
    "* Consistency with the theoretical framework introduced in Notebook 00;\n",
    "* Reproducibility and interpretability of the modeling pipeline.\n",
    "\n",
    "Feature selection is not treated as a purely statistical optimization step, but as a **governance-aware design decision**.\n",
    "\n",
    "It's worth noting here:\n",
    "\n",
    "> All functionalities used in this notebook are derived from behavior or the model. It does not include protected attributes or demographic proxies.\n",
    "\n",
    "\n",
    "### **3.2 Theoretical Feature Set (A–O–S–H Framework)**\n",
    "\n",
    "In alignment with the Autonomous Risk framework, we restrict the model inputs to **theoretical features** derived from system structure rather than raw socio-demographic attributes.\n",
    "\n",
    "The selected features are:\n",
    "\n",
    "$\n",
    "X = {A, A^2, O, H, \\log(1 + S), A \\times O, A^2 \\times \\log(1 + S)}\n",
    "$\n",
    "\n",
    "where:\n",
    "\n",
    "* $A$ denotes **Autonomy;**\n",
    "* $O$ denotes **Opacity;**\n",
    "* $H$ denotes **Human Oversight;**\n",
    "* $S$ denotes **Behavioral Instability.**\n",
    "\n",
    "These features capture **non-linear interactions** and **second-order effects** that are central to autonomous risk emergence.\n",
    "\n",
    "\n",
    "### **3.3 Feature Matrix and Target Vector**\n",
    "\n",
    "Formally, we define:\n",
    "\n",
    "$\n",
    "X \\in \\mathbb{R}^{n \\times p}, \\quad y \\in {0,1}^n\n",
    "$\n",
    "\n",
    "where:\n",
    "\n",
    "* $n$ is the number of observations;\n",
    "* $p = 7$ is the number of selected features;\n",
    "* $y$ corresponds to the target variable `label_default` defined in Section 2.\n",
    "\n",
    "\n",
    "### **Scope and Relationship to the Empirical Notebook**\n",
    "\n",
    "\n",
    "> This notebook documents the canonical implementation of the credit risk modeling pipeline used throughout the study, with an emphasis on interpretable baseline models and their role in the construction of autonomy, opacity, supervision, and instability proxies. While the accompanying empirical notebook extends these analyses to higher-capacity models and richer interaction regimes for stress-testing autonomous risk under increased nonlinearity, the methodological foundations, feature definitions, evaluation metrics, and diagnostic signals remain identical. This separation reflects a deliberate design choice: the present notebook prioritizes conceptual clarity and traceability, while the empirical notebook explores robustness and regime behavior under expanded modeling assumptions.\n",
    "\n",
    "\n",
    "### **3.4 Implementation**\n",
    "\n",
    "```python\n",
    "\n",
    "# Section 3 - Feature Selection and Train/Test Split\n",
    "\n",
    "print(\">>> SECTION 3 - Feature Selection Initiated\\n\")\n",
    "\n",
    "# 3.1 Theoretical Features (A-O-S-H)\n",
    "\n",
    "features_theory = [\n",
    "    \"A\",\n",
    "    \"A_sq\",\n",
    "    \"O\",\n",
    "    \"H\",\n",
    "    \"log1pS\",\n",
    "    \"A_times_O\",\n",
    "    \"A2_logS\"\n",
    "]\n",
    "\n",
    "# Check feature availability\n",
    "for f in features_theory:\n",
    "    if f not in df.columns:\n",
    "        raise KeyError(f\"Missing required feature: {f}\")\n",
    "\n",
    "print(\"Selected theoretical features:\")\n",
    "print(features_theory, \"\\n\")\n",
    "\n",
    "# 3.2 Feature matrix X and target vector y\n",
    "\n",
    "X = df[features_theory].copy()\n",
    "y = df[\"label_default\"].copy()\n",
    "\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape, \"\\n\")\n",
    "\n",
    "# 3.3 Stratified Train/Test Split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.30,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# 3.4 Class Distribution Check\n",
    "\n",
    "print(\"Class distribution in TRAIN set:\")\n",
    "print(y_train.value_counts(normalize=True), \"\\n\")\n",
    "\n",
    "print(\"Class distribution in TEST set:\")\n",
    "print(y_test.value_counts(normalize=True), \"\\n\")\n",
    "\n",
    "assert y_train.nunique() == 2, \"ERROR: Training set contains only one class!\"\n",
    "assert y_test.nunique() == 2, \"ERROR: Test set contains only one class!\"\n",
    "\n",
    "print(\">>> Section 3 completed successfully. Dataset ready for modeling.\\n\")\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "### **3.5 Governance and Scientific Rationale**\n",
    "\n",
    "Key design decisions in this section include:\n",
    "\n",
    "* **Exclusion of socio-demographic variables**, preventing direct or proxy discrimination;\n",
    "* **Use of interaction terms**, enabling detection of nonlinear risk amplification;\n",
    "* **Stratified splitting**, preserving class balance and statistical validity.\n",
    "\n",
    "This ensures that any observed risk patterns emerge from **systemic behavior**, not from sensitive personal attributes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d443b191-35e2-4e55-a2fd-f55f22aef2fe",
   "metadata": {},
   "source": [
    "## **Section 4 - Supervised Models for Credit Risk**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fe18b8-0ec6-4f18-b047-6125354eac35",
   "metadata": {},
   "source": [
    "### **4.1 Objective**\n",
    "\n",
    "This section implements baseline **supervised learning models** to estimate credit default risk using the theoretical feature set defined previously.\n",
    "\n",
    "The goals are to:\n",
    "\n",
    "* Establish predictive baselines under controlled conditions;\n",
    "* Compare linear and non-linear decision mechanisms;\n",
    "* Evaluate whether autonomous risk signals emerge **despite acceptable predictive performance.**\n",
    "\n",
    "Importantly, these models are **not optimized for maximum accuracy**, but for **structural interpretability and governance analysis**.\n",
    "\n",
    "\n",
    "\n",
    "### **4.2 Models Considered**\n",
    "\n",
    "Two standard and widely adopted classifiers are used:\n",
    "\n",
    "#### **Logistic Regression (LR)**\n",
    "\n",
    "* Linear decision boundary;\n",
    "* High interpretability;\n",
    "* Serves as a transparent baseline.\n",
    "\n",
    "#### **Random Forest (RF)**\n",
    "\n",
    "* Non-linear ensemble model;\n",
    "* Captures interaction effects implicitly;\n",
    "* Serves as a proxy for higher opacity systems.\n",
    "\n",
    "This contrast mirrors a core theme of the project: **performance vs. opacity trade-offs**.\n",
    "\n",
    "\n",
    "### **4.3 Model Training**\n",
    "\n",
    "```python\n",
    "\n",
    "# Section 4 - Supervised Models for Credit Risk\n",
    "\n",
    "print(\">>> SECTION 4 - Model Training Initiated\\n\")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 4.3.1 Logistic Regression\n",
    "\n",
    "log_model = LogisticRegression(\n",
    "    max_iter=3000,\n",
    "    solver=\"lbfgs\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "log_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Logistic Regression trained successfully.\")\n",
    "\n",
    "# 4.3.2 Random Forest\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=None,\n",
    "    min_samples_split=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Random Forest trained successfully.\\n\")\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### **4.4 Prediction and Probability Estimates**\n",
    "\n",
    "```python\n",
    "\n",
    "# 4.4 Probability Predictions\n",
    "\n",
    "y_proba_log = log_model.predict_proba(X_test)[:, 1]\n",
    "y_proba_rf  = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Prediction probabilities generated.\")\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "### **4.5 Evaluation Metrics**\n",
    "\n",
    "We evaluate model performance using:\n",
    "\n",
    "* **ROC-AUC:** discrimination ability;\n",
    "* **PR-AUC:** robustness under class imbalance.\n",
    "\n",
    "```python\n",
    "\n",
    "# 4.5 Model Evaluation\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "roc_log = roc_auc_score(y_test, y_proba_log)\n",
    "roc_rf  = roc_auc_score(y_test, y_proba_rf)\n",
    "\n",
    "pr_log = average_precision_score(y_test, y_proba_log)\n",
    "pr_rf  = average_precision_score(y_test, y_proba_rf)\n",
    "\n",
    "print(\"ROC-AUC:\")\n",
    "print(f\"Logistic Regression: {roc_log:.4f}\")\n",
    "print(f\"Random Forest:      {roc_rf:.4f}\\n\")\n",
    "\n",
    "print(\"PR-AUC:\")\n",
    "print(f\"Logistic Regression: {pr_log:.4f}\")\n",
    "print(f\"Random Forest:      {pr_rf:.4f}\")\n",
    "\n",
    "```\n",
    "\n",
    "### **4.6 Interpretation**\n",
    "\n",
    "Several observations are expected at this stage:\n",
    "\n",
    "1. **Random Forest typically outperforms Logistic Regression** in ROC and PR metrics due to its ability to model non-linearities;\n",
    "2. Improved performance comes at the cost of **reduced interpretability and increased opacity;**\n",
    "3. Even when both models achieve acceptable predictive scores, this does **not guarantee governance safety.**\n",
    "\n",
    "This reinforces the central claim of the project:\n",
    "\n",
    "> *Predictive success is neither necessary nor sufficient to ensure low autonomous risk.*\n",
    "\n",
    "\n",
    "### **4.7 Connection to Autonomous Risk Theory**\n",
    "\n",
    "This section provides the **empirical baseline** upon which autonomous risk indicators will later be layered:\n",
    "\n",
    "* Model confidence → Autonomy (A);\n",
    "* Model complexity → Opacity (O);\n",
    "* Human review mechanisms → Oversight (H).\n",
    "\n",
    "Subsequent notebooks will demonstrate how **risk can increase even as predictive metrics improve**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfcd528-6d97-4886-a1c5-225b5daa6d54",
   "metadata": {},
   "source": [
    "## **Section 5 - Model Interpretability and Opacity Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c55105-4e54-4508-be50-5e76e1af579a",
   "metadata": {},
   "source": [
    "### **5.1 Objective**\n",
    "\n",
    "This section evaluates **how interpretable the trained models are**, and how interpretability (or lack thereof) relates to **opacity (O),** a core component of autonomous risk.\n",
    "\n",
    "Rather than treating interpretability as a binary property, we adopt a **graded, operational view**, where opacity increases as:\n",
    "\n",
    "* decision logic becomes less transparent;\n",
    "* feature interactions become harder to disentangle;\n",
    "* explanations become unstable or model-dependent.\n",
    "\n",
    "\n",
    "\n",
    "### **5.2 Why Interpretability Matters for Autonomous Risk**\n",
    "\n",
    "A model can be:\n",
    "\n",
    "* highly accurate;\n",
    "* statistically stable;\n",
    "* and still **unsafe from a governance perspective**.\n",
    "\n",
    "Opacity amplifies autonomous risk because:\n",
    "\n",
    "* it weakens human oversight;\n",
    "* it delays detection of failure modes;\n",
    "* it obscures feedback-loop effects.\n",
    "\n",
    "Thus, opacity is treated as a **risk multiplier**, not merely an inconvenience.\n",
    "\n",
    "\n",
    "\n",
    "### **5.3 Global Feature Importance (Baseline Opacity Signal)**\n",
    "\n",
    "We begin with **global feature importance**, which answers:\n",
    "\n",
    "> *Which features matter most on average?*\n",
    "\n",
    "\n",
    "#### **5.3.1 Logistic Regression - Coefficient Magnitudes**\n",
    "\n",
    "```python\n",
    "\n",
    "# 5.3 - Global Interpretability\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Logistic Regression coefficients\n",
    "coef_df = pd.DataFrame({\n",
    "    \"feature\": X_train.columns,\n",
    "    \"coefficient\": log_model.coef_[0]\n",
    "})\n",
    "\n",
    "coef_df[\"abs_coef\"] = coef_df[\"coefficient\"].abs()\n",
    "coef_df = coef_df.sort_values(\"abs_coef\", ascending=False)\n",
    "\n",
    "coef_df\n",
    "```\n",
    "\n",
    "**Interpretation**:\n",
    "\n",
    "* Coefficients are **directly interpretable;**\n",
    "* Sign and magnitude have clear semantic meaning;\n",
    "* This model exhibits **low opacity**.\n",
    "\n",
    "\n",
    "\n",
    "#### **5.3.2 Random Forest - Gini Importance**\n",
    "\n",
    "```python\n",
    "\n",
    "# Random Forest feature importance\n",
    "rf_importance = pd.DataFrame({\n",
    "    \"feature\": X_train.columns,\n",
    "    \"importance\": rf_model.feature_importances_\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "rf_importance\n",
    "\n",
    "```\n",
    "\n",
    "**Interpretation**:\n",
    "\n",
    "* Importance is aggregated across trees;\n",
    "* No directionality (positive/negative);\n",
    "* Interactions are implicit;\n",
    "* Opacity is **moderate to high**.\n",
    "\n",
    "\n",
    "\n",
    "### **5.4 Local Interpretability via SHAP**\n",
    "\n",
    "To evaluate **local explanations**, we use **SHAP (SHapley Additive exPlanations)**.\n",
    "\n",
    "SHAP allows us to:\n",
    "\n",
    "* decompose individual predictions;\n",
    "* compare explanation stability across models;\n",
    "* quantify opacity empirically.\n",
    "\n",
    "\n",
    "#### **5.4.1 SHAP for Logistic Regression**\n",
    "\n",
    "```python\n",
    "\n",
    "import shap\n",
    "\n",
    "# Use a subset for efficiency\n",
    "X_shap = X_test.sample(500, random_state=42)\n",
    "\n",
    "explainer_log = shap.Explainer(log_model, X_train)\n",
    "shap_values_log = explainer_log(X_shap)\n",
    "\n",
    "shap.summary_plot(\n",
    "    shap_values_log,\n",
    "    X_shap,\n",
    "    show=False\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "**Observation**:\n",
    "\n",
    "* Explanations align closely with coefficients;\n",
    "* Feature effects are stable;\n",
    "* Low variance across samples → **low opacity**.\n",
    "\n",
    "\n",
    "\n",
    "#### **5.4.2 SHAP for Random Forest**\n",
    "\n",
    "```python\n",
    "\n",
    "explainer_rf = shap.Explainer(rf_model, X_train)\n",
    "shap_values_rf = explainer_rf(X_shap)\n",
    "\n",
    "shap.summary_plot(\n",
    "    shap_values_rf,\n",
    "    X_shap,\n",
    "    show=False\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "**Observation**:\n",
    "\n",
    "* Nonlinear patterns emerge;\n",
    "* Feature interactions dominate;\n",
    "* Contribution signs may flip across contexts;\n",
    "* Opacity is **significantly higher**.\n",
    "\n",
    "\n",
    "\n",
    "### **5.5 Opacity Proxy Construction**\n",
    "\n",
    "To operationalize opacity, we define a **model-level opacity proxy**:\n",
    "\n",
    "$$O \\propto \\text{Variance of local explanations across samples}$$\n",
    "\n",
    "\n",
    "#### **5.5.1 Empirical Opacity Score (SHAP Variance)**\n",
    "\n",
    "```python\n",
    "\n",
    "# 5.5 - Opacity Proxy\n",
    "\n",
    "# Extract SHAP values for positive class (Random Forest)\n",
    "shap_vals_rf = shap_values_rf.values\n",
    "\n",
    "# Mean variance across features\n",
    "opacity_proxy_rf = np.mean(np.var(shap_vals_rf, axis=0))\n",
    "\n",
    "opacity_proxy_rf\n",
    "\n",
    "```\n",
    "\n",
    "This scalar captures:\n",
    "\n",
    "* instability of explanations;\n",
    "* sensitivity to local context;\n",
    "* difficulty of governance.\n",
    "\n",
    "Higher values ⇒ higher opacity.\n",
    "\n",
    "\n",
    "\n",
    "### **5.6 Interpretation and Theoretical Link**\n",
    "\n",
    "This section empirically confirms a central theoretical claim:\n",
    "\n",
    "> **Opacity is not binary. It increases smoothly with model complexity and interaction depth.**\n",
    "\n",
    "Key insights:\n",
    "\n",
    "* Logistic Regression → low opacity, high auditability;\n",
    "* Random Forest → higher opacity, even when performance improves;\n",
    "* SHAP variance provides a **measurable governance-relevant signal.**\n",
    "\n",
    "Opacity thus becomes a **first-class variable** in the autonomous risk function.\n",
    "\n",
    "\n",
    "\n",
    "### **5.7 Transition to Autonomous Risk Modeling**\n",
    "\n",
    "At this point, we have:\n",
    "\n",
    "| Component     | Operationalized              |\n",
    "| ------------- | ---------------------------- |\n",
    "| Autonomy (A)  | via confidence & persistence |\n",
    "| Opacity (O)   | via SHAP variance            |\n",
    "| Oversight (H) | via external constraints     |\n",
    "| Performance   | via ROC / PR                 |\n",
    "\n",
    "<br>\n",
    "\n",
    "The next sections will show how **risk escalates when these components interact**, even without catastrophic prediction errors.\n",
    "\n",
    "### **Status Summary**\n",
    "\n",
    "* Interpretability analyzed;  \n",
    "* Opacity quantified;  \n",
    "* Model confidence measured;  \n",
    "* Theoretical linkage established.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1be2944-247a-42f3-af00-9d4c6af9f28d",
   "metadata": {},
   "source": [
    "## **Section 6 - Model Evaluation, Calibration, and Risk Signals**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c5370e-c228-42d1-a723-a70b602d0631",
   "metadata": {},
   "source": [
    "### **6.1 Evaluation Metrics**\n",
    "\n",
    "To ensure robustness beyond accuracy, we evaluate supervised credit risk models using complementary metrics:\n",
    "\n",
    "* **ROC–AUC:** Discriminative capacity across thresholds;\n",
    "* **PR–AUC:** Performance under class imbalance;\n",
    "* **Brier Score:** Calibration-sensitive probabilistic error.\n",
    "\n",
    "These metrics jointly assess predictive quality and probabilistic reliability.\n",
    "\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "\n",
    "# Predicted probabilities\n",
    "y_proba_log = log_mod.predict_proba(X_test)[:, 1]\n",
    "y_proba_rf  = rf_mod.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Logistic ROC-AUC:\", roc_auc_score(y_test, y_proba_log))\n",
    "print(\"RandomForest ROC-AUC:\", roc_auc_score(y_test, y_proba_rf))\n",
    "\n",
    "print(\"Logistic PR-AUC:\", average_precision_score(y_test, y_proba_log))\n",
    "print(\"RandomForest PR-AUC:\", average_precision_score(y_test, y_proba_rf))\n",
    "\n",
    "print(\"Logistic Brier:\", brier_score_loss(y_test, y_proba_log))\n",
    "print(\"RandomForest Brier:\", brier_score_loss(y_test, y_proba_rf))\n",
    "\n",
    "```\n",
    "\n",
    "### **6.2 Calibration Analysis**\n",
    "\n",
    "Well-calibrated models are critical in high-stakes decision-making. Even accurate models may induce systemic risk if their confidence estimates are misaligned.\n",
    "\n",
    "```python\n",
    "from sklearn.calibration import calibration_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "prob_true_log, prob_pred_log = calibration_curve(y_test, y_proba_log, n_bins=10)\n",
    "prob_true_rf, prob_pred_rf = calibration_curve(y_test, y_proba_rf, n_bins=10)\n",
    "\n",
    "plt.figure(figsize=(7,6))\n",
    "plt.plot(prob_pred_log, prob_true_log, marker='o', label='Logistic')\n",
    "plt.plot(prob_pred_rf, prob_true_rf, marker='s', label='Random Forest')\n",
    "plt.plot([0,1],[0,1],'--', color='gray')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Observed Frequency')\n",
    "plt.legend()\n",
    "plt.title('Calibration Curves')\n",
    "plt.show()\n",
    "\n",
    "```\n",
    "\n",
    "Calibration gaps are interpreted as **latent autonomy signals,** where model confidence may exceed epistemic reliability.\n",
    "\n",
    "\n",
    "### **6.3 Instability and Drift Signals**\n",
    "\n",
    "Beyond point estimates, we evaluate behavioral stability across perturbations using:\n",
    "\n",
    "* Variance of predicted probabilities;\n",
    "* Sensitivity to feature noise;\n",
    "* Divergence between models.\n",
    "\n",
    "```python\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "instability_log = np.var(y_proba_log)\n",
    "instability_rf  = np.var(y_proba_rf)\n",
    "\n",
    "print(\"Prediction variance - Logistic:\", instability_log)\n",
    "print(\"Prediction variance - RF:\", instability_rf)\n",
    "\n",
    "```\n",
    "\n",
    "Higher variance under similar inputs is treated as an empirical proxy for **autonomous behavior amplification.**\n",
    "\n",
    "\n",
    "### **6.4 Connection to Autonomous Risk Theory**\n",
    "\n",
    "This section operationalizes key components of the Autonomous Risk framework:\n",
    "\n",
    "* **Autonomy (A):** Captured through confidence concentration and decision persistence;\n",
    "* **Opacity (O):** Reflected by model complexity and interpretability gaps;\n",
    "* **Human Oversight (H):** Assumed fixed but limited in deployment.\n",
    "\n",
    "Empirically, we observe that models with higher capacity exhibit:\n",
    "\n",
    "* Stronger confidence polarization;\n",
    "* Increased calibration drift;\n",
    "* Greater instability under perturbation.\n",
    "\n",
    "These signals precede classical performance degradation and therefore serve as **early warnings of autonomous risk.**\n",
    "\n",
    "\n",
    "\n",
    "### **6.5 Section Summary (Internal)**\n",
    "\n",
    "* Predictive performance evaluated beyond accuracy;\n",
    "* Calibration and probabilistic reliability assessed;\n",
    "* Instability quantified as a precursor to risk;\n",
    "* Empirical results aligned with autonomous risk theory.\n",
    "\n",
    "*This section completes the supervised risk evaluation pipeline and prepares the ground for feedback, opacity, and governance analysis in subsequent notebooks.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe2f126-bf91-43c7-bbbb-1b372181b12f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TensorFlow 3.11)",
   "language": "python",
   "name": "tf_env_311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
