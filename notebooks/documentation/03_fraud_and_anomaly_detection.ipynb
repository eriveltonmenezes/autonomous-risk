{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d48058d2-7986-46ba-b149-e7f033a1bcb5",
   "metadata": {},
   "source": [
    "# **Notebook 03 - Fraud & Anomaly Detection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a405b3-aa58-4683-9339-76d88373843d",
   "metadata": {},
   "source": [
    "> The documented code segments are presented in canonical form for conceptual clarity. The empirical notebook instantiates these mechanisms under concrete data distributions, preserving structural equivalence rather than line-by-line identity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9bfb19-66b5-4706-a97a-e0fc6824f0e8",
   "metadata": {},
   "source": [
    "## **Section 1 - Motivation and Problem Framing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e60c615-c75f-4eea-a3d7-b30c4d9755cb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **1.1 Why Fraud Detection Is Not Just Another Classification Task**\n",
    "\n",
    "Fraud detection differs fundamentally from classical credit risk modeling. While credit default prediction focuses on estimating the probability of an adverse but *expected* outcome, fraud detection targets **rare, adversarial, and adaptive behaviors**. These behaviors are not only infrequent but also strategically shaped to evade detection mechanisms.\n",
    "\n",
    "In practical systems, fraud is characterized by:\n",
    "\n",
    "* Severe class imbalance;\n",
    "* Non-stationary patterns;\n",
    "* Adversarial adaptation to deployed models;\n",
    "* High cost asymmetry between false positives and false negatives.\n",
    "\n",
    "As a result, traditional performance-centric evaluation (accuracy, ROC-AUC alone) is insufficient to characterize the true risk profile of antifraud systems.\n",
    "\n",
    "\n",
    "### **1.2 Fraud as Behavioral and Structural Risk**\n",
    "\n",
    "Unlike default risk, which is often driven by socioeconomic and financial constraints, fraud emerges from **behavioral deviation** and **intentional manipulation of system rules**. Even in synthetic or simulated environments, fraud-like signals can be understood as *out-of-distribution behaviors* relative to the dominant population.\n",
    "\n",
    "From a systems perspective, fraud represents:\n",
    "\n",
    "* A violation of assumed data-generating processes;\n",
    "* A stress test for model robustness;\n",
    "* An early indicator of autonomy-driven risk escalation.\n",
    "\n",
    "This makes fraud detection an ideal empirical setting to study **autonomous risk**, as defined in Notebook 00.\n",
    "\n",
    "\n",
    "### **1.3 Supervised vs Unsupervised Perspectives on Fraud**\n",
    "\n",
    "Fraud detection systems typically combine multiple paradigms:\n",
    "\n",
    "* **Supervised models**, trained on historical labels, excel at detecting known fraud patterns but struggle with novelty.\n",
    "* **Unsupervised and semi-supervised models**, such as anomaly detectors, capture deviations from normal behavior but lack semantic grounding.\n",
    "\n",
    "Above all, neither paradigm alone is sufficient. Fraud emerges at the intersection of *prediction*, *uncertainty*, and *behavioral instability*.\n",
    "\n",
    "This notebook deliberately integrates both perspectives to expose risk signals that remain invisible when models are evaluated in isolation.\n",
    "\n",
    "\n",
    "### **1.4 Connection to Autonomous Risk**\n",
    "\n",
    "Fraud systems are often deployed in continuous feedback loops:\n",
    "\n",
    "* Model outputs influence transaction approvals;\n",
    "* Decisions alter user behavior;\n",
    "* New data reflects prior automated interventions.\n",
    "\n",
    "In such loops, a system may remain accurate while becoming increasingly **opaque**, **self-reinforcing**, or **unstable under drift**.\n",
    "\n",
    "This creates a critical insight:\n",
    "\n",
    "> *Fraud risk is not only about detecting malicious actors, it is also about detecting when the system itself becomes a source of autonomous risk.*\n",
    "\n",
    "Thus, this notebook treats fraud detection as both:\n",
    "\n",
    "* an applied machine learning task, and;\n",
    "* an experimental probe into the dynamics of autonomy, instability, and governance.\n",
    "\n",
    "\n",
    "### **1.5 Objectives of This Notebook**\n",
    "\n",
    "The goals of Notebook 03 are to:\n",
    "\n",
    "1. Construct supervised and unsupervised antifraud models on a consistent synthetic dataset;\n",
    "2. Quantify uncertainty, drift, and instability signals;\n",
    "3. Visualize latent risk regimes through dimensionality reduction;\n",
    "4. Define operational indicators of autonomous risk in antifraud systems;\n",
    "5. Establish a conceptual and empirical bridge toward governance and opacity analysis (Notebook 04).\n",
    "\n",
    "This notebook marks the transition from *risk as prediction error* to **risk as emergent system behavior**.\n",
    "\n",
    "\n",
    "\n",
    "*In the next section, we formalize the dataset, labels, and threat model used throughout the antifraud analysis.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520177c3-2fd9-4aa0-a149-90f7eb52a154",
   "metadata": {},
   "source": [
    "## **Section 2 - Dataset, Fraud Label, and Threat Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bf1929-13e2-4547-93b9-9017e4890c82",
   "metadata": {},
   "source": [
    "This section formalizes how *fraud* is represented in the dataset, how it differs conceptually and operationally from credit default, and why anomaly-based reasoning is essential for this notebook.\n",
    "\n",
    "### **2.1 Dataset Overview**\n",
    "\n",
    "The experiments in this notebook rely on the synthetic financial dataset constructed and refined in **Notebook 01** and subsequently used in **Notebook 02.** The dataset contains individual-level financial, transactional, and behavioral attributes, augmented with theoretical and diagnostic features introduced throughout the project.\n",
    "\n",
    "Key characteristics:\n",
    "\n",
    "* Mixed feature space (financial ratios, transactional behavior, synthetic risk indicators);\n",
    "* Presence of both *supervised labels and unsupervised signals;*\n",
    "* Designed to support stress-testing of governance, opacity, and autonomy under controlled conditions.\n",
    "    \n",
    "Importantly, the dataset is **not intended to model real-world fraud patterns directly,** but to provide a controlled environment for studying *risk amplification and detection limits.*\n",
    "\n",
    "### **2.2 Fraud vs. Credit Default**\n",
    "\n",
    "A central distinction of this notebook is the separation between:\n",
    "\n",
    "* **Credit default risk (Notebook 02):** a relatively stable, outcome-based phenomenon;\n",
    "* **Fraud risk (Notebook 03):** a dynamic, adversarial, and often evasive phenomenon.\n",
    "\n",
    "While default risk can often be estimated from static covariates and historical repayment behavior, fraud typically exhibits the following properties:\n",
    "\n",
    "* Rarity and class imbalance;\n",
    "* Non-stationarity and concept drift;\n",
    "* Strategic adaptation to detection mechanisms;\n",
    "* Weak or noisy labels.\n",
    "\n",
    "As a consequence, fraud detection cannot rely exclusively on classical supervised learning.\n",
    "\n",
    "### **2.3 Target Variable: Fraud Label**\n",
    "\n",
    "The primary supervised target in this notebook is the binary variable:\n",
    "\n",
    "`fraude_simulada`\n",
    "\n",
    "This label represents a *synthetic fraud signal* generated during data construction. It should be interpreted as:\n",
    "\n",
    "> an indicator of potentially malicious or non-compliant behavior, rather than confirmed fraud.\n",
    "\n",
    "Formally, the label is defined as:\n",
    "\n",
    "$$Y_{fraud} \\in {0,1}$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $Y_{fraud} = 1$ denotes suspicious or fraudulent behavior;\n",
    "* $Y_{fraud} = 0$ denotes normal behavior.\n",
    "\n",
    "This abstraction reflects real-world constraints, where ground truth fraud labels are often delayed, uncertain, or incomplete.\n",
    "\n",
    "### **2.4 Threat Model Assumptions**\n",
    "\n",
    "To contextualize the experiments, we adopt the following simplified threat model:\n",
    "\n",
    "1. Fraudulent behavior is **rare but high-impact;**\n",
    "2. Attackers (or anomalous agents) adapt their behavior over time;\n",
    "3. Detection systems may influence future data through feedback loops;\n",
    "4. Not all risky behavior is fraudulent, and not all fraud appears risky under static rules.\n",
    "\n",
    "These assumptions motivate the combined use of:\n",
    "\n",
    "* Supervised classifiers;\n",
    "* Unsupervised anomaly detectors;\n",
    "* Stability, drift, and uncertainty indicators.\n",
    "\n",
    "### **2.5 Why Anomaly Detection Is Necessary**\n",
    "\n",
    "In contrast to credit risk modeling, fraud detection must address scenarios where:\n",
    "\n",
    "* Labels are unreliable or missing;\n",
    "* Novel patterns emerge beyond the training distribution;\n",
    "* The cost of false negatives is disproportionately high.\n",
    "\n",
    "Therefore, this notebook explicitly integrates:\n",
    "\n",
    "* Isolation-based methods;\n",
    "* Reconstruction-based methods (autoencoders);\n",
    "* Hybrid risk indicators combining prediction, uncertainty, and instability.\n",
    "\n",
    "This shift marks the transition from *predictive risk* to **behavioral and autonomous risk,** which will be further explored in subsequent sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43a8359-df04-48ad-95ed-ab08ca5ab826",
   "metadata": {},
   "source": [
    "## **Section 3 - Feature Selection and Experimental Design**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9385fd-84c1-4dcd-8be1-9c1994a2fee5",
   "metadata": {},
   "source": [
    "This section describes how features are selected and organized for fraud detection, and how the experimental design reflects the hybrid nature of antifraud systems, combining supervised and unsupervised components.\n",
    "\n",
    "### **3.1 Feature Space Overview**\n",
    "\n",
    "The dataset contains a heterogeneous set of features capturing financial status, transactional behavior, and synthetic risk indicators. For the purposes of fraud and anomaly detection, features are grouped into three conceptual categories:\n",
    "\n",
    "1. **Baseline Financial Features:** These include income proxies, debt ratios, credit utilization measures, and other static or slowly varying attributes;\n",
    "\n",
    "2. **Behavioral and Transactional Features:** Variables capturing frequency, intensity, and irregularity of actions (e.g., transaction volume variability, abrupt changes in behavior);\n",
    "\n",
    "3. **Derived and Theoretical Risk Indicators:** Synthetic features introduced in previous notebooks, such as interaction terms and non-linear transformations, designed to amplify latent instability and stress-test model robustness.\n",
    "\n",
    "This structured feature space allows the comparison between *semantic risk signals and purely statistical anomalies.*\n",
    "\n",
    "### **3.2 Feature Selection Rationale**\n",
    "\n",
    "Unlike classical supervised learning, fraud detection does not aim to optimize predictive performance alone. Instead, feature selection is guided by the following principles:\n",
    "\n",
    "* **Sensitivity to deviation** rather than central tendency;\n",
    "* **Robustness to class imbalance;**\n",
    "* **Ability to support unsupervised learning;**\n",
    "* **Interpretability under governance constraints.**\n",
    "\n",
    "Therefore, highly redundant or trivially predictive features are not necessarily preferred, especially if they obscure behavioral instability or inflate confidence artificially.\n",
    "\n",
    "### **3.3 Supervised vs Unsupervised Feature Usage**\n",
    "\n",
    "The feature set is intentionally reused across different modeling paradigms:\n",
    "\n",
    "* Supervised models use the full feature set to learn known fraud patterns;\n",
    "* Unsupervised models (e.g., Isolation Forest, Autoencoders) treat the same features as a representation of “normal behavior” and identify deviations.\n",
    "\n",
    "This design choice allows us to:\n",
    "\n",
    "* Compare risk signals across paradigms;\n",
    "* Detect disagreements between classifiers and anomaly detectors;\n",
    "* Identify regions of high autonomy and low interpretability.\n",
    "\n",
    "### **3.4 Train/Test Strategy and Temporal Considerations**\n",
    "\n",
    "Fraud detection systems are particularly sensitive to **temporal leakage** and **concept drift.** To mitigate these effects, the experimental design follows these guidelines:\n",
    "\n",
    "* Data splits respect temporal ordering when applicable;\n",
    "* Supervised models are evaluated under class imbalance-aware metrics;\n",
    "* Unsupervised models are calibrated only on presumed normal data.\n",
    "\n",
    "This setup approximates real-world antifraud deployment, where models must generalize to evolving and partially adversarial environments.\n",
    "\n",
    "### **3.5 Experimental Axes of Analysis**\n",
    "\n",
    "Rather than focusing on a single performance metric, the experiments in this notebook are organized along multiple analytical axes:\n",
    "\n",
    "* **Predictive performance** (precision, recall, ROC-AUC);\n",
    "* **Anomaly sensitivity** (outlier scores, reconstruction error);\n",
    "* **Uncertainty and instability;**\n",
    "* **Drift across simulated regimes;**\n",
    "* **Opacity and interpretability constraints.**\n",
    "\n",
    "These axes collectively support the central objective: **characterizing autonomous risk beyond prediction accuracy.**\n",
    "\n",
    "### **3.6 Link to Autonomous Risk Framework**\n",
    "\n",
    "The experimental design of this notebook explicitly aligns with the autonomous risk framework introduced in Notebook 00:\n",
    "\n",
    "* Feature interactions may amplify autonomy;\n",
    "* Unsupervised detectors reveal governance blind spots;\n",
    "* Disagreements between models signal instability;\n",
    "* Persistent anomalies under feedback loops indicate emergent risk.\n",
    "\n",
    "Thus, feature selection and experimental design are not neutral preprocessing steps, they are *structural components* of autonomous risk analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1ebbd5-a1fe-4a47-a46b-6a7a0aa197fd",
   "metadata": {},
   "source": [
    "## **Section 4 - Supervised Models for Fraud Detection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3560de22-bdbe-4953-a7f8-8f5ddc1068b4",
   "metadata": {},
   "source": [
    "This section introduces supervised learning models for fraud detection and clarifies their role, strengths, and limitations within the broader autonomous risk framework.\n",
    "\n",
    "### **4.1 Role of Supervised Learning in Antifraud Systems**\n",
    "\n",
    "Supervised models constitute the backbone of most industrial antifraud systems. They are trained on historical labels and optimized to *recognize previously observed* fraudulent behaviors.\n",
    "\n",
    "Their primary strengths include:\n",
    "\n",
    "* High precision on known fraud patterns;\n",
    "* Direct optimization toward operational objectives;\n",
    "* Compatibility with regulatory and audit requirements.\n",
    "\n",
    "However, these models inherently assume **stationarity** and **label completeness,** two assumptions that rarely hold in adversarial environments.\n",
    "\n",
    "### **4.2 Target Variable Definition**\n",
    "\n",
    "The supervised task focuses on predicting the binary fraud indicator:\n",
    "\n",
    "$$\n",
    "\\text{y}_i =\n",
    "\\begin{cases}\n",
    "1 & \\text{if} \\text {transaction } i \\text{ is fraudulent} \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "This label represents *observed fraud,* not total fraud. Undetected or adaptive behaviors remain unlabeled, reinforcing the need for complementary unsupervised analysis.\n",
    "\n",
    "### **4.3 Baseline Supervised Models**\n",
    "\n",
    "Two supervised classifiers are considered:\n",
    "\n",
    "1. **Logistic Regression**\n",
    "\n",
    "* Serves as a transparent baseline;\n",
    "* Offers direct interpretability through coefficients;\n",
    "* Provides calibrated probability outputs.\n",
    "\n",
    "2. **Tree-Based Ensemble Model (Random Forest)**\n",
    "\n",
    "* Captures non-linear interactions;\n",
    "* Handles heterogeneous features effectively;\n",
    "* More expressive, but less transparent.\n",
    "\n",
    "The contrast between these models allows us to analyze the **accuracy–opacity trade-off,** a core dimension of autonomous risk.\n",
    "\n",
    "### **4.4 Class Imbalance and Evaluation Metrics**\n",
    "\n",
    "Fraud datasets are typically highly imbalanced. As a result:\n",
    "\n",
    "* Accuracy is not a meaningful metric;\n",
    "* ROC-AUC alone may be misleading;\n",
    "* Precision–Recall metrics are emphasized.\n",
    "\n",
    "Evaluation focuses on:\n",
    "\n",
    "* Precision at relevant recall levels;\n",
    "* PR-AUC;\n",
    "* Stability of predictions across subsets.\n",
    "\n",
    "This choice reflects real-world antifraud priorities, where false positives incur operational costs and false negatives enable harm.\n",
    "\n",
    "### **4.5 Calibration and Confidence Signals**\n",
    "\n",
    "Beyond raw predictions, supervised models emit **confidence signals** (estimated probabilities). These signals are critical inputs for downstream decision-making and risk aggregation.\n",
    "\n",
    "However, high confidence does not imply correctness under:\n",
    "\n",
    "* Concept drift;\n",
    "* Adversarial adaptation;\n",
    "* Feedback-induced distribution shifts.\n",
    "\n",
    "Therefore, confidence itself becomes a **risk-relevant variable,** later integrated into autonomous risk indicators.\n",
    "\n",
    "### **4.6 Limitations of Supervised Fraud Detection**\n",
    "\n",
    "Despite strong performance on historical data, supervised models exhibit structural limitations:\n",
    "\n",
    "* Inability to detect novel fraud strategies;\n",
    "* Sensitivity to label noise and delayed feedback;\n",
    "* Overconfidence in sparse regions of feature space.\n",
    "\n",
    "These limitations motivate the introduction of **unsupervised anomaly detection,** addressed in the next section.\n",
    "\n",
    "### **4.7 Connection to Autonomous Risk**\n",
    "\n",
    "From the autonomous risk perspective:\n",
    "\n",
    "* Supervised models optimize *local prediction objectives;*\n",
    "* They may increase system-level risk by reinforcing learned patterns;\n",
    "* High accuracy can coexist with growing opacity and instability.\n",
    "\n",
    "Thus, supervised fraud detection is necessary but insufficient. It must be embedded within a broader framework that monitors uncertainty, drift, and emergent behaviors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387ad548-e9d7-415c-a318-156a7658fbac",
   "metadata": {},
   "source": [
    "## **Section 5 - Unsupervised Anomaly Detection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc8187a-f2ee-4e96-b17a-5bbfccac2148",
   "metadata": {},
   "source": [
    "This section introduces unsupervised anomaly detection as a complementary lens to supervised fraud models, focusing on novelty, deviation, and latent instability.\n",
    "\n",
    "### **5.1 Why Unsupervised Detection Is Essential in Fraud Systems**\n",
    "\n",
    "Fraud is inherently adaptive. Once a supervised model is deployed, adversarial actors adjust their behavior to evade known detection patterns. As a result, fraud systems face a persistent *unknown unknowns problem.*\n",
    "\n",
    "Unsupervised anomaly detection addresses this gap by:\n",
    "\n",
    "* Modeling normal behavior without relying on labels;\n",
    "* Identifying rare or structurally deviant observations;\n",
    "* Remaining sensitive to emerging fraud strategies.\n",
    "    \n",
    "Rather than predicting fraud directly, these models estimate **behavioral abnormality.**\n",
    "\n",
    "### **5.2 Conceptual Definition of Anomaly**\n",
    "\n",
    "An anomaly is defined as an observation that deviates significantly from the dominant data-generating process:\n",
    "\n",
    "$$\n",
    "\\text{Anomaly}(x_i) \\Longleftrightarrow p(x_i) \\ll p(x) \n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "This definition is *distributional,* not semantic. An anomaly is not necessarily fraudulent, but persistent anomalies often signal:\n",
    "\n",
    "* Behavioral manipulation;\n",
    "* Data drift;\n",
    "* Model blind spots.\n",
    "\n",
    "### **5.3 Isolation Forest**\n",
    "\n",
    "Isolation Forest detects anomalies by measuring how easily observations are isolated in random partition trees.\n",
    "\n",
    "Key properties:\n",
    "\n",
    "* Does not assume any specific data distribution;\n",
    "* Efficient for high-dimensional data;\n",
    "* Particularly effective for sparse fraud signals.\n",
    "\n",
    "The anomaly score reflects **path length,** with shorter paths indicating higher abnormality.\n",
    "\n",
    "### **5.4 Autoencoders for Latent Reconstruction Error**\n",
    "\n",
    "Autoencoders learn compressed representations of normal behavior by minimizing reconstruction error:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(x) = |x - \\hat{x}|^2\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "High reconstruction error suggests that an observation lies outside the learned manifold.\n",
    "\n",
    "This approach captures:\n",
    "\n",
    "* Complex non-linear dependencies;\n",
    "* Latent structural deviations;\n",
    "* Subtle anomalies missed by tree-based methods.\n",
    "    \n",
    "However, autoencoders introduce additional opacity and training sensitivity.\n",
    "\n",
    "### **5.5 Anomaly Scores as Risk Signals**\n",
    "\n",
    "Unsupervised models produce **continuous anomaly scores,** not binary decisions. These scores are treated as *risk indicators* rather than direct fraud labels.\n",
    "\n",
    "They are later integrated with:\n",
    "\n",
    "* Supervised fraud probabilities;\n",
    "* Confidence and calibration signals;\n",
    "* Opacity and interpretability metrics.\n",
    "    \n",
    "This integration reflects the project’s core principle: **risk emerges from interactions, not isolated predictions.**\n",
    "\n",
    "### **5.6 Limitations of Unsupervised Detection**\n",
    "\n",
    "Despite their advantages, unsupervised methods face important constraints:\n",
    "\n",
    "* Sensitivity to feature scaling;\n",
    "* Difficulty distinguishing benign novelty from malicious behavior;\n",
    "* Lack of semantic explanation.\n",
    "\n",
    "Therefore, anomaly detection must be interpreted contextually and never used in isolation.\n",
    "\n",
    "### **5.7 Connection to Autonomous Risk**\n",
    "\n",
    "From the autonomous risk perspective, anomaly detectors serve as **early warning sensors:**\n",
    "\n",
    "* They detect when the system’s learned representation no longer aligns with incoming data;\n",
    "* They expose latent instability before accuracy degrades;\n",
    "* They highlight regions where supervision is weakest.\n",
    "\n",
    "In feedback-driven systems, rising anomaly scores may indicate **emergent autonomy** rather than external threat alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae93887f-63cb-4fee-a2ff-cd7be7a23bba",
   "metadata": {},
   "source": [
    "## **Section 6 - Integrating Supervised and Unsupervised Signals**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7894d7-bc2e-4e65-9365-fdf8644170c5",
   "metadata": {},
   "source": [
    "This section formalizes the integration of supervised fraud predictions and unsupervised anomaly signals, moving from isolated model outputs to **system-level risk indicators.**\n",
    "\n",
    "### **6.1 Why Signal Integration Is Necessary**\n",
    "\n",
    "Neither supervised nor unsupervised models alone can fully characterize fraud risk:\n",
    "\n",
    "* Supervised models capture known fraud patterns but are blind to novelty;\n",
    "* Unsupervised models detect deviation but lack semantic grounding.\n",
    "\n",
    "In real-world systems, risk emerges from inconsistencies between signals, not from absolute scores.\n",
    "\n",
    "Thus, the central question becomes:\n",
    "\n",
    "> *What does it mean when a transaction is considered normal by a classifier but highly anomalous by a behavioral model?*\n",
    "\n",
    "\n",
    "### 6.2 Formal Signal Definitions\n",
    "\n",
    "For each transaction $i$, we define:\n",
    "\n",
    "<center><b>Supervised fraud probability:<b></center>\n",
    "\n",
    "$$P_i^{\\text{fraud}} = \\mathbb{P}(y_i = 1 \\mid x_i)$$\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><b>Anomaly score (normalized):<b></center>\n",
    "\n",
    "$$A_i^{\\text{anom}} \\in [0,1]$$\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><b> Prediction confidence or stability:<b></center>\n",
    "\n",
    "$$C_i = 1 - H(p_i)$$\n",
    "\n",
    "<br>\n",
    "\n",
    "where $H(\\cdot)$ denotes predictive entropy.\n",
    "\n",
    "\n",
    "### **6.3 Risk Escalation Through Signal Divergence**\n",
    "\n",
    "We define **signal divergence** as:\n",
    "\n",
    "$$D_i = \\left| P_i^{\\text{fraud}} - A_i^{\\text{anom}} \\right|$$\n",
    "\n",
    "<br>\n",
    "\n",
    "High divergence indicates internal disagreement within the system.\n",
    "\n",
    "This disagreement often emerges:\n",
    "\n",
    "* Under data drift;\n",
    "* During adversarial adaptation;\n",
    "* When models overfit historical regimes.\n",
    "\n",
    "\n",
    "### **6.4 Composite Autonomous Risk Indicator**\n",
    "\n",
    "We define a composite risk indicator:\n",
    "\n",
    "$$R_i^{\\text{auto}} = P_i^{\\text{fraud}} \\cdot A_i^{\\text{anom}} \\cdot (1 - C_i)$$\n",
    "\n",
    "<br>\n",
    "\n",
    "This formulation captures three simultaneous conditions:\n",
    "\n",
    "1. Elevated fraud likelihood;\n",
    "2. Structural behavioral deviation;\n",
    "3. Reduced epistemic confidence.\n",
    "\n",
    "Only when all three align does **autonomous risk** escalate.\n",
    "\n",
    "\n",
    "### **6.5 Regime-Based Interpretation**\n",
    "\n",
    "Transactions can be grouped into regimes:\n",
    "\n",
    "| Regime | Supervised Risk | Anomaly | Confidence | Interpretation           |\n",
    "| ------ | --------------- | ------- | ---------- | ------------------------ |\n",
    "| I      | Low             | Low     | High       | Normal operation         |\n",
    "| II     | High            | Low     | High       | Known fraud patterns     |\n",
    "| III    | Low             | High    | Low        | Emerging behavior        |\n",
    "| IV     | High            | High    | Low        | Critical autonomous risk |\n",
    "\n",
    "Regime IV represents the most dangerous zone: decisions are confident enough to act, but unstable enough to mislead.\n",
    "\n",
    "\n",
    "### **6.6 Visualization of Risk Regimes**\n",
    "\n",
    "These regimes are later visualized via:\n",
    "\n",
    "* 2D and 3D projections;\n",
    "* Temporal trajectories;\n",
    "* Risk surface plots.\n",
    "\n",
    "Such visualizations transform abstract risk into **governable structures.**\n",
    "\n",
    "\n",
    "### **6.7 Connection to Feedback Loops**\n",
    "\n",
    "In deployed systems, these indicators feed back into:\n",
    "\n",
    "* Transaction blocking;\n",
    "* Customer friction;\n",
    "* Model retraining triggers.\n",
    "\n",
    "Unchecked, this feedback can amplify instability, a hallmark of autonomous risk.\n",
    "Thus, integration is not merely analytical; it is **preventive governance.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6f893e-f74f-4164-867b-696f285cc833",
   "metadata": {},
   "source": [
    "## **Section 7 - Dimensionality Reduction and Latent Risk Geometry**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc089cf-cff1-4108-b1f0-eacb6666f4d1",
   "metadata": {},
   "source": [
    "This section explores the **latent geometric structure** of fraud and anomaly signals through dimensionality reduction techniques. The objective is not visualization alone, but **risk regime discovery.**\n",
    "\n",
    "### **7.1 Why Geometry Matters in Fraud Systems**\n",
    "\n",
    "Fraud detection operates in high-dimensional feature spaces where:\n",
    "\n",
    "* Human intuition fails;\n",
    "* Linear separability is rare;\n",
    "* Risk emerges from *combinations,* not individual variables.\n",
    "\n",
    "Dimensionality reduction allows us to:\n",
    "\n",
    "* Reveal hidden behavioral regimes;\n",
    "* Detect transitions between stable and unstable zones;\n",
    "* Observe clustering induced by autonomous feedback loops.\n",
    "\n",
    "Geometry, here, is a diagnostic tool.\n",
    "\n",
    "### **7.2 Feature Space for Projection**\n",
    "\n",
    "The projection space combines:\n",
    "\n",
    "* Core behavioral features;\n",
    "* Supervised fraud probability;\n",
    "* Unsupervised anomaly scores;\n",
    "* Uncertainty-related indicators.\n",
    "\n",
    "Formally, each transaction is represented as:\n",
    "\n",
    "<center>$z_i = [x_i, P_i^{\\text{fraud}}, A_i^{\\text{anom}}, C_i]$</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "This enriched representation embeds decision, deviation, and confidence simultaneously.\n",
    "\n",
    "\n",
    "### **7.3 Principal Component Analysis (PCA)**\n",
    "\n",
    "PCA is first applied to:\n",
    "\n",
    "* Capture dominant variance directions;\n",
    "* Identify linear risk gradients;\n",
    "* Detect global structure.\n",
    "\n",
    "Key observations:\n",
    "\n",
    "* The first components often align with transaction volume and behavioral intensity;\n",
    "* Fraud labels do not necessarily align with principal variance directions;\n",
    "* High-risk regimes may reside in low-variance subspaces.\n",
    "\n",
    "This already hints at **hidden risk.**\n",
    "\n",
    "\n",
    "### **7.4 Nonlinear Projections (UMAP / t-SNE)**\n",
    "\n",
    "To capture nonlinear structure, we apply UMAP (or t-SNE):\n",
    "\n",
    "* Preserves local neighborhoods;\n",
    "* Reveals manifold structure;\n",
    "* Highlights transitional zones between regimes.\n",
    "\n",
    "Empirically, we observe:\n",
    "\n",
    "* Dense clusters of normal behavior;\n",
    "* Peripheral regions dominated by anomalies;\n",
    "* Overlapping zones where supervised and unsupervised signals conflict.\n",
    "\n",
    "These overlapping regions correspond to **autonomous risk escalation.**\n",
    "\n",
    "### **7.5 Visualizing Risk Regimes**\n",
    "\n",
    "Points are color-coded by:\n",
    "\n",
    "* Fraud probability;\n",
    "* Anomaly score;\n",
    "* Composite autonomous risk $R^{\\text{auto}}$.\n",
    "\n",
    "This reveals:\n",
    "\n",
    "* Smooth risk gradients rather than sharp boundaries;\n",
    "* Risk “funnels” where uncertainty increases;\n",
    "* Structural asymmetries induced by model feedback.\n",
    "\n",
    "### **7.6 Latent Instability and Early Warning Zones**\n",
    "\n",
    "Certain regions exhibit:\n",
    "\n",
    "* High anomaly but low fraud probability;\n",
    "* Rapid shifts over time;\n",
    "* Sparse data density.\n",
    "\n",
    "These zones act as **early warning indicators:**\n",
    "\n",
    "* Not yet fraud;\n",
    "* Not yet errors;\n",
    "* But structurally unstable.\n",
    "\n",
    "Traditional evaluation metrics do not capture this.\n",
    "\n",
    "### **7.7 Geometry as Governance Instrument**\n",
    "\n",
    "Latent geometry enables:\n",
    "\n",
    "* Regime-based monitoring;\n",
    "* Targeted human review;\n",
    "* Adaptive thresholds based on region, not global scores.\n",
    "\n",
    "This reframes governance:\n",
    "\n",
    "> *From thresholding outputs to supervising structures.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ae9897-f130-47bf-a11f-2f7f1a15c118",
   "metadata": {},
   "source": [
    "## **Section 8 - Uncertainty, Drift, and Temporal Risk Dynamics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1ec37c-f382-409f-a04c-2321c4d150cb",
   "metadata": {},
   "source": [
    "This section analyzes fraud risk as a **dynamic process,** not a static classification outcome. We focus on **uncertainty, distributional drift,** and their interaction over time as drivers of autonomous risk escalation.\n",
    "\n",
    "### **8.1 Why Static Risk Is Insufficient**\n",
    "\n",
    "Most fraud models assume:\n",
    "\n",
    "* Fixed data distributions;\n",
    "* Stable decision boundaries;\n",
    "* Stationary behavior.\n",
    "\n",
    "However, real antifraud systems operate in environments where:\n",
    "\n",
    "* User behavior adapts;\n",
    "* Attack strategies evolve;\n",
    "* Model decisions alter future data.\n",
    "\n",
    "Thus, risk must be understood temporally.\n",
    "\n",
    "### **8.2 Predictive Uncertainty as a Risk Signal**\n",
    "\n",
    "Uncertainty captures how confident a model is in its own predictions.\n",
    "\n",
    "We analyze uncertainty through:\n",
    "\n",
    "* Prediction entropy;\n",
    "* Variability across models or bootstrap runs;\n",
    "* Instability in probability outputs.\n",
    "\n",
    "High uncertainty indicates:\n",
    "\n",
    "* Sparse data regions;\n",
    "* Novel behavior;\n",
    "* Potential regime transitions.\n",
    "\n",
    "Primarily:\n",
    "\n",
    "> **Low error does not imply low uncertainty.**\n",
    "\n",
    "### **8.3 Drift Detection and Distributional Change**\n",
    "\n",
    "Drift quantifies how current data diverges from past reference distributions.\n",
    "We consider:\n",
    "\n",
    "* Feature-level drift;\n",
    "* Prediction drift;\n",
    "* Latent space drift (post-PCA/UMAP).\n",
    "\n",
    "Drift is not inherently bad, but unmanaged drift is dangerous.\n",
    "\n",
    "Observed patterns:\n",
    "\n",
    "* Gradual drift in transaction volume;\n",
    "* Abrupt drift linked to behavioral manipulation;\n",
    "* Drift amplification through feedback loops.\n",
    "\n",
    "### **8.4 Interaction Between Uncertainty and Drift**\n",
    "\n",
    "The most critical risk zones occur when:\n",
    "\n",
    "* Drift is increasing and\n",
    "* Uncertainty remains high or rising.\n",
    "\n",
    "This interaction signals:\n",
    "\n",
    "* Loss of epistemic grounding;\n",
    "* Model overconfidence collapse;\n",
    "* Emergent autonomy of system behavior.\n",
    "\n",
    "Formally, autonomous risk increases when:\n",
    "\n",
    "$$\\frac{dU}{dt} > 0 \\quad \\text{and} \\quad \\frac{dD}{dt} > 0$$\n",
    "\n",
    "<br>\n",
    "\n",
    "### **8.5 Temporal Visualization of Risk Escalation**\n",
    "\n",
    "By tracking uncertainty and drift over time, we observe:\n",
    "\n",
    "* Stable regimes with bounded fluctuation;\n",
    "* Transitional regimes with oscillatory behavior;\n",
    "* Runaway regimes with monotonic escalation.\n",
    "\n",
    "These trajectories expose **system-level failure modes** before classical metrics degrade.\n",
    "\n",
    "\n",
    "### **8.6 Early Warning Indicators**\n",
    "\n",
    "Key early signals include:\n",
    "\n",
    "* Rising uncertainty in low-error regions;\n",
    "* Drift concentrated in specific latent clusters;\n",
    "* Increasing disagreement between supervised and unsupervised signals.\n",
    "\n",
    "These indicators precede:\n",
    "\n",
    "* Fraud outbreaks;\n",
    "* Model collapse;\n",
    "* Governance failures.\n",
    "\n",
    "### **8.7 From Monitoring to Intervention**\n",
    "\n",
    "Temporal risk analysis enables:\n",
    "\n",
    "* Adaptive retraining schedules;\n",
    "* Region-specific thresholds;\n",
    "* Human-in-the-loop escalation triggered by dynamics, not scores.\n",
    "\n",
    "This shifts antifraud systems from **reactive** to **preventive.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc9f182-cbfa-4058-9f89-f15b17055fb3",
   "metadata": {},
   "source": [
    "## **Section 9 - Autonomous Risk Synthesis and Governance Implications**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7874d7-afca-4acc-9696-f21e9965b6ae",
   "metadata": {},
   "source": [
    "This final section synthesizes the empirical findings of the antifraud experiments and situates them within the broader theory of **Autonomous Risk** developed throughout the project.\n",
    "\n",
    "\n",
    "### **9.1 Fraud Systems as Autonomous Risk Amplifiers**\n",
    "\n",
    "The experiments demonstrate that antifraud systems can exhibit increasing risk **even when predictive performance remains stable.**\n",
    "\n",
    "This occurs when:\n",
    "\n",
    "* Decision feedback loops reshape the data distribution;\n",
    "* Models adapt faster than governance mechanisms;\n",
    "* Uncertainty and drift escalate silently.\n",
    "\n",
    "Thus, antifraud systems may act not only as risk detectors, but as **risk amplifiers.**\n",
    "\n",
    "\n",
    "### **9.2 Risk Beyond Accuracy and Compliance**\n",
    "\n",
    "Traditional governance frameworks focus on:\n",
    "\n",
    "* Accuracy thresholds;\n",
    "* Bias metrics;\n",
    "* Static compliance checks.\n",
    "\n",
    "However, our results show that:\n",
    "\n",
    "* Risk emerges dynamically;\n",
    "* Instability can accumulate under the surface;\n",
    "* Models can become opaque without explicit design flaws.\n",
    "\n",
    "This reveals a fundamental limitation of static governance.\n",
    "\n",
    "\n",
    "### **9.3 Autonomous Risk as a System-Level Property**\n",
    "\n",
    "Autonomous risk is not attributable to:\n",
    "\n",
    "* A single model;\n",
    "* A specific dataset;\n",
    "* A particular algorithm.\n",
    "\n",
    "Instead, it arises from:\n",
    "\n",
    "* Model–environment interaction;\n",
    "* Feedback-driven adaptation;\n",
    "* Partial observability and delayed supervision.\n",
    "\n",
    "In antifraud systems, this manifests as:\n",
    "\n",
    "* Drift-driven escalation;\n",
    "* Confidence decoupled from correctness;\n",
    "* Latent regime transitions.\n",
    "\n",
    "\n",
    "### **9.4 Implications for AI Governance**\n",
    "\n",
    "Effective governance must evolve from:\n",
    "\n",
    "> **“Is the model accurate?”**\n",
    "> \n",
    "to:\n",
    "> \n",
    "> **“Is the system dynamically stable?”**\n",
    "\n",
    "This implies:\n",
    "\n",
    "* Continuous monitoring of uncertainty and drift;\n",
    "* Intervention policies based on trajectories, not snapshots;\n",
    "* Explicit limits on autonomy in high-impact regimes.\n",
    "\n",
    "\n",
    "### **9.5 Operationalizing Governance Controls**\n",
    "\n",
    "Based on the findings, governance should include:\n",
    "\n",
    "* Autonomous risk dashboards;\n",
    "* Early warning indicators;\n",
    "* Escalation thresholds tied to system dynamics;\n",
    "* Human oversight triggered by instability signals.\n",
    "\n",
    "These controls target *emergent behavior,* not isolated predictions.\n",
    "\n",
    "\n",
    "### **9.6 Positioning Within the Broader Project**\n",
    "\n",
    "Notebook 03 establishes:\n",
    "\n",
    "* Fraud detection as a stress-test environment;\n",
    "* Empirical evidence of autonomous risk dynamics;\n",
    "* A bridge between prediction and governance.\n",
    "\n",
    "This prepares the foundation for:\n",
    "\n",
    "* Opacity and control analysis (Notebook 04);\n",
    "* Feedback loops and scheming (Notebook 05);\n",
    "* AGI safety extensions (Notebook 06).\n",
    "\n",
    "### **9.7 Key Takeaways**\n",
    "\n",
    "* Fraud risk is dynamic and adaptive;\n",
    "* Uncertainty and drift are early indicators of failure;\n",
    "* Autonomous risk emerges before observable collapse;\n",
    "* Governance must address systems, not models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3576b8-123b-45dd-a4c1-34fd69e155a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TensorFlow 3.11)",
   "language": "python",
   "name": "tf_env_311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
