{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "377eac31-d8ff-4ae0-b3c6-781c68f602ef",
   "metadata": {},
   "source": [
    "# **Notebook 01 - Synthetic Environments and Feature Construction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c35d1ef-fd5b-4ab3-9e0b-ad102009eba0",
   "metadata": {},
   "source": [
    "## **Section 1 - Purpose of the Synthetic Environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1692bb-1e0f-41ba-adf9-10e3a75feb73",
   "metadata": {},
   "source": [
    "### **1.1 Why a Synthetic Environment**\n",
    "\n",
    "The empirical study of autonomous risk in intelligent systems presents a fundamental methodological challenge: real-world datasets embed **uncontrolled confounders, historical biases, regulatory artifacts, and socio-economic correlations** that obscure the structural mechanisms under investigation.\n",
    "\n",
    "If the objective is to understand *how* autonomous risk emerges (rather than merely *where* it appears) then a controlled experimental setting becomes indispensable.\n",
    "\n",
    "For this reason, this notebook adopts a **synthetic data paradigm**.\n",
    "\n",
    "Synthetic environments allow us to:\n",
    "\n",
    "* Explicitly define causal and correlational structures;\n",
    "* Isolate the effects of autonomy, opacity, and feedback;\n",
    "* Introduce controlled noise and uncertainty;\n",
    "* Test hypothetical governance regimes without ethical or legal risk.\n",
    "\n",
    "The goal is not realism per se, but **structural fidelity**: the environment must reproduce the *mechanisms* through which risk arises, not necessarily the surface statistics of any specific industry dataset.\n",
    "\n",
    "### **Methodological Scope Clarification**\n",
    "\n",
    "This synthetic environment is not designed to support population-level inference or external validity claims. Its sole purpose is structural identification: \n",
    "\n",
    ">isolating and stress-testing the causal mechanisms through which autonomous risk, opacity, and governance failure can emerge.\n",
    "\n",
    "Consequently, the validity of this dataset should be evaluated in terms of causal clarity and internal coherence, not empirical representativeness.\n",
    "\n",
    "\n",
    "\n",
    "### **1.2 Role of the Synthetic Dataset in the Project**\n",
    "\n",
    "The dataset generated in this notebook serves as the **common substrate** for all subsequent notebooks.\n",
    "\n",
    "Specifically, it will be used to:\n",
    "\n",
    "* Train and evaluate supervised risk models (Notebook 02);\n",
    "* Simulate anomaly and fraud detection pipelines (Notebook 03);\n",
    "* Analyze opacity, interpretability, and auditability (Notebook 04);\n",
    "* Study feedback loops, instability, and scheming indicators (Notebook 05);\n",
    "* Support extensions toward AI Safety and AGI-aligned risk analysis (Notebook 06).\n",
    "\n",
    "Any flaw, ambiguity, or inconsistency introduced at this stage would propagate downstream.\n",
    "Accordingly, **this notebook prioritizes clarity, traceability, and reproducibility over sophistication**.\n",
    "\n",
    "\n",
    "\n",
    "### **1.3 What This Environment Is, and Is Not**\n",
    "\n",
    "To avoid conceptual drift, we explicitly delimit the scope of the synthetic environment.\n",
    "\n",
    "This environment **is**:\n",
    "\n",
    "* A controlled experimental laboratory;\n",
    "* A generator of structured, interpretable signals;\n",
    "* A testbed for autonomy-related risk dynamics.\n",
    "\n",
    "This environment **is not**:\n",
    "\n",
    "* A faithful representation of real populations;\n",
    "* A benchmark for predictive performance;\n",
    "* A substitute for domain-specific datasets.\n",
    "\n",
    "No claim is made that models trained here are deployable in real-world settings.\n",
    "\n",
    "Their value lies exclusively in **explanatory and analytical insight**.\n",
    "\n",
    "\n",
    "\n",
    "### **1.4 Ethical and Normative Considerations**\n",
    "\n",
    "All variables generated in this notebook are either:\n",
    "\n",
    "* Abstract behavioral signals;\n",
    "* Financial or transactional aggregates;\n",
    "* Latent theoretical constructs.\n",
    "\n",
    "No sensitive personal attributes (e.g., gender, race, religion, ethnicity) are included as causal drivers.\n",
    "\n",
    "This design choice ensures that:\n",
    "\n",
    "* Observed risk patterns cannot be attributed to protected characteristics;\n",
    "* Ethical and legal concerns do not contaminate structural analysis;\n",
    "* Subsequent discussions of fairness and governance remain conceptually clean.\n",
    "\n",
    "\n",
    "\n",
    "### **1.5 Reproducibility Commitment**\n",
    "\n",
    "All random processes in this notebook will be:\n",
    "\n",
    "* Seeded explicitly;\n",
    "* Documented step by step;\n",
    "* Exported in deterministic formats.\n",
    "\n",
    "This guarantees that:\n",
    "\n",
    "* Independent readers can regenerate identical datasets;\n",
    "* Results across notebooks remain consistent;\n",
    "* Extensions and replications are feasible.\n",
    "\n",
    "\n",
    "\n",
    "*With this foundation established, we now proceed to define the generative assumptions that govern the synthetic environment.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6d7fd4-5137-440e-b40b-cab2f13d29f5",
   "metadata": {},
   "source": [
    "## **Section 2 - Generative Assumptions of the Synthetic Environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b368830-b62a-4c9d-9a94-7d5f98685ce7",
   "metadata": {},
   "source": [
    "### **2.1 Design Philosophy**\n",
    "\n",
    "The synthetic environment is governed by a small set of **explicit generative assumptions**. These assumptions are not meant to reflect empirical truths about any specific domain, but to **instantiate structural conditions under which autonomous risk can be meaningfully studied**.\n",
    "\n",
    "The guiding principle is **minimal sufficiency**:\n",
    "\n",
    "> introduce only those mechanisms strictly necessary to observe autonomy, instability, opacity, and feedback-driven risk.\n",
    "\n",
    "Every variable generated downstream must be traceable to at least one of these assumptions.\n",
    "\n",
    "\n",
    "\n",
    "### **2.2 Population-Level Assumptions**\n",
    "\n",
    "We assume the existence of a population of agents (or accounts, users, entities) characterized by heterogeneous but bounded attributes.\n",
    "\n",
    "Formally:\n",
    "\n",
    "* The population size ( N ) is fixed and known;\n",
    "* Each entity is independent at generation time;\n",
    "* Dependencies emerge only through model-mediated interactions and feedback loops (later notebooks).\n",
    "\n",
    "This ensures that:\n",
    "\n",
    "* Initial correlations are *designed*, not accidental;\n",
    "* Emergent dependencies can be attributed to system behavior rather than data leakage.\n",
    "\n",
    "\n",
    "\n",
    "### **2.3 Latent Behavioral Structure**\n",
    "\n",
    "We posit the existence of latent behavioral traits that are **not directly observable**, but influence multiple observed variables.\n",
    "\n",
    "These latent traits include:\n",
    "\n",
    "* financial regularity vs. volatility;\n",
    "* transactional intensity;\n",
    "* behavioral consistency over time.\n",
    "\n",
    "Importantly:\n",
    "\n",
    "* These latent traits are *not labels;*\n",
    "* They act as **common causes** for multiple observed features.\n",
    "\n",
    "This design allows models to infer structure (and potentially overfit or misinterpret it) creating the conditions for opacity and autonomous behavior.\n",
    "\n",
    "\n",
    "\n",
    "### **2.4 Noise and Imperfection Assumption**\n",
    "\n",
    "All observed variables are assumed to be **noisy projections** of underlying processes.\n",
    "\n",
    "Noise is introduced intentionally to:\n",
    "\n",
    "* prevent trivial separability;\n",
    "* avoid deterministic mappings;\n",
    "* simulate epistemic uncertainty.\n",
    "\n",
    "Noise terms are:\n",
    "\n",
    "* independent across variables;\n",
    "* bounded;\n",
    "* stationary at generation time.\n",
    "\n",
    "This ensures that:\n",
    "\n",
    "* High confidence does not imply correctness;\n",
    "* Model certainty can diverge from ground truth;\n",
    "* Entropy and instability measures become meaningful.\n",
    "\n",
    "\n",
    "\n",
    "### **2.5 Non-Stationarity Readiness**\n",
    "\n",
    "Although the dataset generated in this notebook is static, it is designed to be **compatible with non-stationary extensions**.\n",
    "\n",
    "Specifically:\n",
    "\n",
    "* Feature definitions support temporal slicing;\n",
    "* Aggregates can be recomputed under drift;\n",
    "* Labels can be regenerated under shifting thresholds.\n",
    "\n",
    "This prepares the environment for:\n",
    "\n",
    "* drift detection (Notebook 03);\n",
    "* feedback amplification (Notebook 05);\n",
    "* governance stress tests (Notebook 06).\n",
    "\n",
    "\n",
    "\n",
    "### **2.6 Absence of Intentional Agency**\n",
    "\n",
    "Specifically, **no intentional agency is encoded** at the data generation level.\n",
    "\n",
    "Entities do not:\n",
    "\n",
    "* optimize objectives;\n",
    "* adapt strategies;\n",
    "* respond to incentives.\n",
    "\n",
    "Any appearance of strategic behavior, scheming, or autonomy in later notebooks therefore arises **entirely from the models and deployment dynamics**, not from the data itself.\n",
    "\n",
    "This separation is essential to support the core thesis:\n",
    "\n",
    "> autonomous risk can emerge *without* intentional agents.\n",
    "\n",
    "\n",
    "**Fundamentally, no optimization process, reward signal, or strategic adaptation is present at the data generation stage.**\n",
    "\n",
    "Any subsequent appearance of goal-directed behavior, scheming, or evasive dynamics must therefore arise from model-mediated feedback loops and deployment conditions, not from embedded intent in the dataset itself. \n",
    "\n",
    "This establishes a strict causal boundary between data generation and emergent autonomy.\n",
    "\n",
    "\n",
    "### **2.7 Summary of Assumptions**\n",
    "\n",
    "In summary, the synthetic environment assumes:\n",
    "\n",
    "1. Independent entities at generation time;\n",
    "2. Latent behavioral structure affecting multiple observables;\n",
    "3. Noisy, imperfect measurement;\n",
    "4. Structural readiness for drift and feedback;\n",
    "5. No embedded intent or agency.\n",
    "\n",
    "These assumptions define the **sandbox** within which the remainder of the project operates.\n",
    "\n",
    "\n",
    "\n",
    "*We now translate these assumptions into concrete variables and distributions.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590557ca-aaa0-48ea-9336-67b4805eb85b",
   "metadata": {},
   "source": [
    "## **Section 3 - Base Variables and Population Generation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcae228-a11e-43c5-a707-a1e8e1c554e4",
   "metadata": {},
   "source": [
    "### **3.1 Overview**\n",
    "\n",
    "This section translates the generative assumptions into **concrete variables and distributions**. The goal is not realism in the econometric sense, but **structural plausibility**: variables must interact in ways that allow autonomy, opacity, and risk amplification to emerge downstream.\n",
    "\n",
    "All variables generated here are **exogenous** to the models that will later consume them.\n",
    "\n",
    "\n",
    "\n",
    "### **3.2 Population Size and Reproducibility**\n",
    "\n",
    "We fix a population size (N) and enforce reproducibility through a global random seed.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "N = 10_000\n",
    "```\n",
    "\n",
    "This choice balances:\n",
    "\n",
    "* statistical richness;\n",
    "* computational tractability;\n",
    "* stability across experiments.\n",
    "\n",
    "\n",
    "\n",
    "### **3.3 Demographic and Structural Attributes**\n",
    "\n",
    "We begin with coarse-grained attributes that act as **contextual variables**, not decision drivers.\n",
    "\n",
    "```python\n",
    "idade = np.random.randint(18, 75, size=N)\n",
    "\n",
    "estado_civil = np.random.choice(\n",
    "    [\"single\", \"married\", \"divorced\", \"widowed\"],\n",
    "    size=N,\n",
    "    p=[0.45, 0.38, 0.12, 0.05]\n",
    ")\n",
    "\n",
    "escolaridade = np.random.choice(\n",
    "    [\"basic\", \"high_school\", \"college\", \"postgraduate\"],\n",
    "    size=N,\n",
    "    p=[0.25, 0.40, 0.25, 0.10]\n",
    ")\n",
    "\n",
    "regiao = np.random.choice(\n",
    "    [\"north\", \"south\", \"east\", \"west\", \"central\"],\n",
    "    size=N\n",
    ")\n",
    "```\n",
    "\n",
    "These variables:\n",
    "\n",
    "* introduce heterogeneity;\n",
    "* support later fairness and governance analysis;\n",
    "* are **not** intended to be causal drivers of risk.\n",
    "\n",
    "\n",
    "### **Normative Role of Demographic Variables**\n",
    "\n",
    "Demographic attributes included in this dataset serve exclusively as observational variables for downstream fairness and governance analysis. They are not used as causal drivers, optimization targets, or label-generating features. Their presence enables normative audits without contaminating the structural analysis of autonomous risk.\n",
    "\n",
    "\n",
    "\n",
    "### **3.4 Economic Capacity Proxies**\n",
    "\n",
    "We introduce continuous variables representing **capacity and constraints.**\n",
    "\n",
    "```python\n",
    "renda_estim = np.random.lognormal(mean=8.5, sigma=0.6, size=N)\n",
    "\n",
    "tempo_emprego = np.clip(\n",
    "    np.random.exponential(scale=6, size=N),\n",
    "    0,\n",
    "    40\n",
    ")\n",
    "```\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Lognormal income induces natural skewness;\n",
    "* Employment time introduces stability variation;\n",
    "* These variables will later correlate with behavior, not labels.\n",
    "\n",
    "\n",
    "\n",
    "### **3.5 Financial Instrumentation**\n",
    "\n",
    "We now generate variables that mediate **system exposure**.\n",
    "\n",
    "```python\n",
    "num_cartoes = np.random.randint(1, 6, size=N)\n",
    "\n",
    "limite_total = renda_estim * np.random.uniform(0.5, 2.5, size=N)\n",
    "\n",
    "utilizacao_media = np.clip(\n",
    "    np.random.beta(2, 5, size=N),\n",
    "    0, 1\n",
    ")\n",
    "```\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "* Higher income allows higher limits;\n",
    "* Utilization is bounded and asymmetric;\n",
    "* Capacity ≠ usage (important for anomaly detection).\n",
    "\n",
    "\n",
    "\n",
    "### **3.6 Transactional Behavior**\n",
    "\n",
    "Behavioral intensity and irregularity are modeled independently.\n",
    "\n",
    "```python\n",
    "quant_transacoes = np.random.poisson(lam=30, size=N)\n",
    "\n",
    "valor_medio_trans = np.random.lognormal(mean=4.0, sigma=0.7, size=N)\n",
    "\n",
    "transacoes_incomuns = np.random.binomial(\n",
    "    n=quant_transacoes,\n",
    "    p=0.05\n",
    ")\n",
    "```\n",
    "\n",
    "These variables will later support:\n",
    "\n",
    "* anomaly detection;\n",
    "* instability signals;\n",
    "* emergent scheming indicators.\n",
    "\n",
    "\n",
    "\n",
    "### **3.7 Risk-Related Proxies (Pre-Label)**\n",
    "\n",
    "At this stage, we introduce **risk correlates**, not labels.\n",
    "\n",
    "```python\n",
    "historico_atrasos = np.random.poisson(lam=1.2, size=N)\n",
    "\n",
    "divida_renda = np.clip(\n",
    "    limite_total / (renda_estim + 1e-6),\n",
    "    0, 5\n",
    ")\n",
    "```\n",
    "\n",
    "These variables encode **latent stress**, not outcomes.\n",
    "\n",
    "\n",
    "\n",
    "### **3.8 Assembly of the Base DataFrame**\n",
    "\n",
    "We consolidate all variables into a single DataFrame.\n",
    "\n",
    "```python\n",
    "df = pd.DataFrame({\n",
    "    \"idade\": idade,\n",
    "    \"estado_civil\": estado_civil,\n",
    "    \"escolaridade\": escolaridade,\n",
    "    \"regiao\": regiao,\n",
    "    \"renda_estim\": renda_estim,\n",
    "    \"tempo_emprego\": tempo_emprego,\n",
    "    \"num_cartoes\": num_cartoes,\n",
    "    \"limite_total\": limite_total,\n",
    "    \"utilizacao_media\": utilizacao_media,\n",
    "    \"quant_transacoes\": quant_transacoes,\n",
    "    \"valor_medio_trans\": valor_medio_trans,\n",
    "    \"transacoes_incomuns\": transacoes_incomuns,\n",
    "    \"historico_atrasos\": historico_atrasos,\n",
    "    \"divida_renda\": divida_renda\n",
    "})\n",
    "```\n",
    "\n",
    "Sanity check:\n",
    "\n",
    "```python\n",
    "df.head()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### **3.9 Design Guarantees**\n",
    "\n",
    "At the end of this section, we guarantee that:\n",
    "\n",
    "* No labels exist yet;\n",
    "* No intentional behavior is encoded;\n",
    "* No optimization has occurred;\n",
    "* All structure is latent and noisy.\n",
    "\n",
    "This ensures that **any risk or autonomy detected later is model-induced**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779ca5a0-a88c-432f-8ebe-3d9767509248",
   "metadata": {},
   "source": [
    "## **Section 4 - Synthetic Risk Signals and Label Construction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d3d8bf-e7e1-462d-a791-ae49d60412f6",
   "metadata": {},
   "source": [
    "### **4.1 Rationale**\n",
    "\n",
    "This section introduces **risk-related signals** and **labels** that will later be used by supervised and unsupervised models. The central methodological constraint is the following:\n",
    "\n",
    "> **Labels must emerge from structured interactions between variables, never be trivially encoded in a single feature.**\n",
    "\n",
    "If labels are too obvious, downstream models merely learn shortcuts. If labels are too random, no meaningful structure can emerge. Our objective is a **controlled middle ground**.\n",
    "\n",
    "\n",
    "\n",
    "### **4.2 Latent Risk Score (Unobserved)**\n",
    "\n",
    "We begin by constructing a *latent* continuous risk score. This score is **never directly exposed** to models.\n",
    "\n",
    "```python\n",
    "latent_risk = (\n",
    "    0.35 * df[\"divida_renda\"] +\n",
    "    0.25 * df[\"historico_atrasos\"] +\n",
    "    0.20 * df[\"utilizacao_media\"] +\n",
    "    0.10 * (df[\"transacoes_incomuns\"] / (df[\"quant_transacoes\"] + 1e-6)) +\n",
    "    0.10 * np.random.normal(0, 1, size=N)\n",
    ")\n",
    "```\n",
    "\n",
    "Key properties:\n",
    "\n",
    "* Multi-factor composition;\n",
    "* Noise injected explicitly;\n",
    "* No demographic variables included;\n",
    "* No hard thresholds.\n",
    "\n",
    "\n",
    "\n",
    "### **4.3 Normalized Probability of Adverse Outcome**\n",
    "\n",
    "We convert the latent score into a probability-like signal using a logistic transformation.\n",
    "\n",
    "```python\n",
    "prob_inadimplencia = 1 / (1 + np.exp(-latent_risk))\n",
    "```\n",
    "\n",
    "Add to the DataFrame:\n",
    "\n",
    "```python\n",
    "df[\"prob_inadimplencia\"] = prob_inadimplencia\n",
    "```\n",
    "\n",
    "This variable:\n",
    "\n",
    "* Is continuous;\n",
    "* Represents *risk propensity*, not realization;\n",
    "* Will later be used to derive multiple labels.\n",
    "\n",
    "\n",
    "\n",
    "### **4.4 Primary Label: Credit Default (Binary)**\n",
    "\n",
    "We define a **quantile-based label**, ensuring class balance control without leaking scale information.\n",
    "\n",
    "```python\n",
    "threshold = df[\"prob_inadimplencia\"].quantile(0.90)\n",
    "\n",
    "df[\"label_default\"] = (df[\"prob_inadimplencia\"] > threshold).astype(int)\n",
    "```\n",
    "\n",
    "Sanity check:\n",
    "\n",
    "```python\n",
    "df[\"label_default\"].value_counts(normalize=True)\n",
    "```\n",
    "\n",
    "This ensures approximately **10% positives**, suitable for:\n",
    "\n",
    "* credit risk modeling;\n",
    "* ROC/PR evaluation;\n",
    "* stress-testing autonomy effects.\n",
    "\n",
    "\n",
    "\n",
    "### **4.5 Secondary Label: Simulated Fraud (Orthogonal)**\n",
    "\n",
    "Fraud is modeled as **partially correlated but not identical** to default risk.\n",
    "\n",
    "```python\n",
    "fraude_simulada = (\n",
    "    (df[\"transacoes_incomuns\"] > np.percentile(df[\"transacoes_incomuns\"], 95)) &\n",
    "    (df[\"utilizacao_media\"] > 0.7) &\n",
    "    (np.random.rand(N) < 0.6)\n",
    ").astype(int)\n",
    "\n",
    "df[\"fraude_simulada\"] = fraude_simulada\n",
    "```\n",
    "\n",
    "Important:\n",
    "\n",
    "* Fraud ≠ default;\n",
    "* Some overlap exists, but neither subsumes the other;\n",
    "* Enables multi-task risk analysis later.\n",
    "\n",
    "\n",
    "\n",
    "### **4.6 Behavioral Anomaly Flag (Unsupervised Proxy)**\n",
    "\n",
    "We introduce a **non-label anomaly signal**, to be used in unsupervised settings.\n",
    "\n",
    "```python\n",
    "anomalia_padrao = (\n",
    "    (df[\"quant_transacoes\"] > df[\"quant_transacoes\"].quantile(0.95)) |\n",
    "    (df[\"valor_medio_trans\"] > df[\"valor_medio_trans\"].quantile(0.95))\n",
    ").astype(int)\n",
    "\n",
    "df[\"anomalia_padrao\"] = anomalia_padrao\n",
    "```\n",
    "\n",
    "This variable:\n",
    "\n",
    "* Is intentionally crude;\n",
    "* Represents *surface-level abnormality*;\n",
    "* Will later be contrasted with learned anomalies.\n",
    "\n",
    "\n",
    "\n",
    "### **4.7 Behavioral Suspicion Index (Soft Signal)**\n",
    "\n",
    "We add a soft behavioral flag to capture borderline patterns.\n",
    "\n",
    "```python\n",
    "comportamento_suspeito = (\n",
    "    0.5 * df[\"utilizacao_media\"] +\n",
    "    0.3 * (df[\"transacoes_incomuns\"] / (df[\"quant_transacoes\"] + 1e-6)) +\n",
    "    0.2 * np.random.rand(N)\n",
    ")\n",
    "\n",
    "df[\"comportamento_suspeito\"] = comportamento_suspeito\n",
    "```\n",
    "\n",
    "This variable:\n",
    "\n",
    "* Is continuous;\n",
    "* Is not a label;\n",
    "* Supports later interpretability and risk layering.\n",
    "\n",
    "\n",
    "\n",
    "### **4.8 Final Consistency Checks**\n",
    "\n",
    "```python\n",
    "assert df[\"label_default\"].nunique() == 2\n",
    "assert df[\"fraude_simulada\"].nunique() == 2\n",
    "assert df.isnull().sum().sum() == 0\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### **4.9 Conceptual Guarantees**\n",
    "\n",
    "At the end of this section:\n",
    "\n",
    "* Labels are **derived**, not injected;\n",
    "* Multiple risk notions coexist;\n",
    "* No single feature trivially predicts any label;\n",
    "* Future autonomy and opacity emerge **from modeling**, not data leakage.\n",
    "\n",
    "This is a *clean causal boundary* between data generation and model behavior.\n",
    "\n",
    "### **Interpretation of Opacity (O)**\n",
    "\n",
    "Opacity is operationalized here as a proxy for epistemic distance between system internals and external oversight, not as a direct measure of risk, complexity, or performance. Its construction intentionally combines autonomy and imperfect external signals to reflect auditability loss rather than outcome severity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f940910-fb53-4ee7-8b0a-9b5180e387c7",
   "metadata": {},
   "source": [
    "## **Section 5 - Dataset Finalization, Versioning and Reproducibility**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1abe024-c052-439e-ba25-170787a41fed",
   "metadata": {},
   "source": [
    "### **5.1 Objective of This Section**\n",
    "\n",
    "This section has three clear goals:\n",
    "\n",
    "1. **Freeze** the dataset in a reproducible state;\n",
    "2. **Document** its structure and guarantees;\n",
    "3. **Persist** the dataset in formats suitable for all downstream notebooks.\n",
    "\n",
    "From this point onward:\n",
    "\n",
    "> **No notebook is allowed to mutate the raw dataset.**\n",
    "> All transformations must be explicit, derived, and documented.\n",
    "\n",
    "This is essential for scientific validity and later publication.\n",
    "\n",
    "\n",
    "\n",
    "### **5.2 Final Dataset Overview**\n",
    "\n",
    "Before saving, we inspect the final schema.\n",
    "\n",
    "```python\n",
    "df.columns.tolist()\n",
    "```\n",
    "\n",
    "Expected groups of variables:\n",
    "\n",
    "#### **Profile & Socioeconomic (non-sensitive)**\n",
    "\n",
    "* idade\n",
    "* estado_civil\n",
    "* escolaridade\n",
    "* renda_estim\n",
    "* regiao\n",
    "* tipo_emprego\n",
    "* tempo_emprego\n",
    "\n",
    "#### **Financial & Transactional**\n",
    "\n",
    "* num_cartoes\n",
    "* limite_total\n",
    "* utilizacao_media\n",
    "* historico_atrasos\n",
    "* score_credito\n",
    "* divida_renda\n",
    "* quant_transacoes\n",
    "* valor_medio_trans\n",
    "* compra_internacional\n",
    "* assinaturas\n",
    "* transacoes_incomuns\n",
    "\n",
    "#### **Risk & Behavioral Signals**\n",
    "\n",
    "* prob_inadimplencia\n",
    "* label_default\n",
    "* fraude_simulada\n",
    "* anomalia_padrao\n",
    "* comportamento_suspeito\n",
    "\n",
    "> **No protected attributes** (e.g., sex, race, religion) are present.\n",
    "> This is a deliberate design choice aligned with governance constraints explored later.\n",
    "\n",
    "\n",
    "\n",
    "### **5.3 Dataset Integrity Checks**\n",
    "\n",
    "We perform structural and statistical sanity checks.\n",
    "\n",
    "```python\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Missing values:\", df.isnull().sum().sum())\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(df[\"label_default\"].value_counts(normalize=True))\n",
    "```\n",
    "\n",
    "Optional: correlation snapshot (for human inspection only):\n",
    "\n",
    "```python\n",
    "df[\n",
    "    [\"prob_inadimplencia\", \"label_default\", \"fraude_simulada\", \"anomalia_padrao\"]\n",
    "].corr()\n",
    "```\n",
    "\n",
    "These checks are **diagnostic**, not used by models.\n",
    "\n",
    "\n",
    "\n",
    "### **5.4 Dataset Versioning Strategy**\n",
    "\n",
    "We explicitly version the dataset to prevent silent drift.\n",
    "\n",
    "**Version naming convention:**\n",
    "\n",
    "```\n",
    "DatasetFinanceiro_v3.2\n",
    "```\n",
    "\n",
    "For this notebook:\n",
    "\n",
    "* Sample size: `10000`\n",
    "* Major version: `1`\n",
    "* Minor version: `0`\n",
    "\n",
    "**Version:** `DatasetFinanceiro_v3.2.csv`\n",
    "\n",
    "\n",
    "\n",
    "### **5.5 Persisting the Dataset**\n",
    "\n",
    "We save the dataset in two complementary formats.\n",
    "\n",
    "#### CSV (human-readable, GitHub-friendly)\n",
    "\n",
    "```python\n",
    "df.to_csv(\n",
    "    \"DatasetFinanceiro_v3.2.csv\",\n",
    "    index=False\n",
    ")\n",
    "```\n",
    "\n",
    "#### Parquet (efficient, analytics-friendly)\n",
    "\n",
    "```python\n",
    "df.to_parquet(\n",
    "    \"\"DatasetFinanceiro_v3.2.parquet\",\n",
    "    index=False\n",
    ")\n",
    "```\n",
    "\n",
    "Confirmation:\n",
    "\n",
    "```python\n",
    "print(\"\"DatasetFinanceiro_v3.2 saved successfully.\")\n",
    "df.head()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### **5.6 Dataset Evolution Across the Project**\n",
    "\n",
    "This notebook introduces the base synthetic environment and the core variables used throughout the project. However, it does not represent the final state of the dataset.\n",
    "\n",
    "As the theoretical framework is progressively operationalized, later notebooks introduce additional features, including uncertainty proxies, drift measures, anomaly signals, and higher-order interaction terms.\n",
    "\n",
    "The final dataset used for all consolidated analyses is:\n",
    "\n",
    "> **DatasetFinanceiro_v3.2**\n",
    "\n",
    "Earlier versions generated in this notebook and intermediate stages are maintained for reproducibility but should be interpreted as developmental snapshots rather than final empirical objects.\n",
    "\n",
    "\n",
    "\n",
    "### **5.7 Reproducibility Guarantees**\n",
    "\n",
    "This notebook guarantees:\n",
    "\n",
    "* Deterministic generation (via fixed random seeds);\n",
    "* Explicit feature construction;\n",
    "* Clear separation between:\n",
    "\n",
    "  * raw signals;\n",
    "  * derived risk variables;\n",
    "  * labels.\n",
    "\n",
    "Any future experiment can:\n",
    "\n",
    "* reload this dataset;\n",
    "* recompute derived features;\n",
    "* reproduce all reported results.\n",
    "\n",
    "\n",
    "\n",
    "### **5.8 Formal Closure of Notebook 01**\n",
    "\n",
    "> **Notebook 01 establishes the empirical substrate of the project.**\n",
    "\n",
    "From this point onward:\n",
    "\n",
    "* Risk is no longer *simulated arbitrarily*;\n",
    "* It is *learned, amplified, distorted, or controlled* by models.\n",
    "\n",
    "This allows us to study **autonomous risk as an emergent property**, not a baked-in artifact.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0c0ed4-0d1a-4f21-9009-e9a95db08793",
   "metadata": {},
   "source": [
    "## **Epistemic Role of Notebook 01**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad281368-42ec-4ef0-a269-2eeb859dfc15",
   "metadata": {},
   "source": [
    "This notebook functions as a controlled experimental substrate rather than a predictive benchmark. It establishes the minimal conditions under which autonomous risk can be meaningfully studied, ensuring that all downstream phenomena reflect model behavior and system dynamics rather than artifacts of data leakage or embedded intent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493b44ce-177c-4d88-ae5e-a2f74948c33d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TensorFlow 3.11)",
   "language": "python",
   "name": "tf_env_311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
